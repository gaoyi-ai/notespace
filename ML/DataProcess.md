---
title: Data Process
categories:
- ML
- Data Preprocessing
tags:
- linear interpolation
- EMA
date: 2021/2/1 10:00:00
updated: 2021/2/2 16:00:00
---



# 数据处理

## 离散 -> 连续

### 插值法：最近邻插值

在图像的仿射变换中，很多地方需要用到插值运算，常见的插值运算包括最邻近插值，双线性插值，双三次插值，兰索思插值等方法，OpenCV 提供了很多方法，其中，双线性插值由于折中的插值效果和运算速度，运用比较广泛。   　

越是简单的模型越适合用来举例子，我们就举个简单的图像：3\*3 的 256 级灰度图。假如图像的像素矩阵如下图所示（这个原始图把它叫做源图，Source）：
234 38 22       
67 44 12       
89 65 63   　
如果想把这副图放大为 4\*4 大小的图像，那么该怎么做呢？那么第一步肯定想到的是先把 4\*4 的矩阵先画出来再说，好了矩阵画出来了，如下所示，当然，矩阵的每个像素都是未知数，等待着我们去填充（这个将要被填充的图的叫做目标图, Destination）：
? ? ? ?   　
? ? ? ?   　
? ? ? ?   　
? ? ? ?   　
然后要往这个空的矩阵里面填值了，要填的值从哪里来来呢？是从源图中来，好，先填写目标图最左上角的像素，坐标为（0，0），那么该坐标对应源图中的坐标可以由如下公式得出 `srcX=dstX* (srcWidth/dstWidth)` , `srcY = dstY * (srcHeight/dstHeight)  ` 　

利用公式，就可以找到对应的原图的坐标了 `(0*(3/4),0*(3/4))=>(0*0.75,0*0.75)=>(0,0)`，找到了源图的对应坐标, 就可以把源图中坐标为(0,0) 处的 234 像素值填进去目标图的 (0,0) 这个位置了。   

接下来, 如法炮制, 寻找目标图中坐标为 (1,0) 的像素对应源图中的坐标, 套用公式:   `(1*0.75,0*0.75)=>(0.75,0)` 结果发现, 得到的坐标里面竟然有小数, 这可怎么办? 计算机里的图像可是数字图像, 像素就是最小单位了, 像素的坐标都是整数, 从来没有小数坐标。这时候采用的一种策略就是采用四舍五入的方法（也可以采用直接舍掉小数位的方法），把非整数坐标转换成整数，好，那么按照四舍五入的方法就得到坐标（1，0），完整的运算过程就是这样的：`(1*0.75,0*0.75)=>(0.75,0)=>(1,0) `那么就可以再填一个像素到目标矩阵中了，同样是把源图中坐标为 (1,0) 处的像素值 38 填入目标图中的坐标。

依次填完每个像素，一幅放大后的图像就诞生了，像素矩阵如下所示：
234 38 22 22   　
67 44 12 12   　
89 65 63 63   　
89 65 63 63   　
这种放大图像的方法叫做最临近插值算法，这是一种最基本、最简单的图像缩放算法，效果也是最不好的，放大后的图像有很严重的马赛克，缩小后的图像有很严重的失真；效果不好的根源就是其简单的最临近插值方法引入了严重的图像失真，比如，当由目标图的坐标反推得到的源图的的坐标是一个浮点数的时候，采用了四舍五入的方法，直接采用了和这个浮点数最接近的像素的值，这种方法是很不科学的，当推得坐标值为 0.75 的时候，不应该就简单的取为 1，既然是 0.75，比 1 要小 0.25 ，比 0 要大 0.75 , 那么目标像素值其实应该根据这个源图中虚拟的点四周的四个真实的点来按照一定的规律计算出来的，这样才能达到更好的缩放效果。   

双线型内插值算法就是一种比较好的图像缩放算法，它充分的利用了源图中虚拟点四周的四个真实存在的像素值来共同决定目标图中的一个像素值，因此缩放效果比简单的最邻近插值要好很多。  

### 插值法：线性插值

在数学中，线性插值是一种使用线性多项式进行曲线拟合的方法，可以在一组离散的已知数据点范围内构造新的数据点。

#### Linear interpolation between two known points

>![img](https://gitee.com/gaoyi-ai/image-bed/raw/master/images/220px-Linear_interpolation_visualisation.svg.png)
>
>In this geometric visualisation, the value at the green circle multiplied by the horizontal distance between the red and blue circles is equal to the sum of the value at the red circle multiplied by the horizontal distance between the green and blue circles, and the value at the blue circle multiplied by the horizontal distance between the green and red circles.

如果两个已知点是由坐标给出的$(x_{0}，y_{0})$和$(x_1,y_1)$，线性插值是这些点之间的直线。 对于间隔$(x_0, x_1)$中的值x，沿直线的y值由斜率方程式给出
$$
\frac{y - y_0}{x - x_0} = \frac{y_1 - y_0}{x_1 - x_0},
$$
可以从右侧的图以几何图形得出。 这是n = 1的多项式插值的一种特殊情况。

求解该方程的y（x的未知值），得出
$$
y = y_0 + (x-x_0)\frac{y_1 - y_0}{x_1 - x_0} = \frac{y_0(x_1 - x)+y_1(x - x_0)}{x_1 - x_0},
$$
这是区间$(x_0,x_1)$中线性插值的公式。 在此间隔之外，公式与线性外推相同。

该公式也可以理解为加权平均值。 权重与从端点到未知点的距离成反比。 较近的点比较远的点有更大的影响。 因此，权重为$1 - \frac{x-x_0}{x_1-x_0}$和$1 - \frac{x-x_0}{x_1-x_0}$，这是未知点和每个端点之间的标准化距离。 因为这些总和为1，
$$
y = y_0 \left(1 - \frac{x - x_0}{x_1 - x_0}\right) + y_1 \left(1 - \frac{x_1 - x}{x_1 - x_0}\right) =
  y_0 \left(1 - \frac{x - x_0}{x_1 - x_0}\right) + y_1 \left(\frac{x - x_0}{x_1 - x_0}\right),
$$
得出上面给出的线性插值公式。

工程上，通常y=ax^3^+bx^2^+cx+d,每4个点拟合出一条曲线 -> '三色样条'

#### Linear interpolation as approximation

线性插值常用于用函数f在其他点的两个已知值来逼近函数f的值。这种近似的误差定义为
$$
R_T = f(x) - p(x),
$$
其中p表示上面定义的线性插值多项式：
$$
p(x) = f(x_0) + \frac{f(x_1) - f(x_0)}{x_1 - x_0}(x - x_0).
$$
利用罗尔定理可以证明，如果f有连续的二阶导数，则误差有界于
$$
|R_T| \leq \frac{(x_1 - x_0)^2}{8} \max_{x_0 \leq x \leq x_1} |f''(x)|.
$$
也就是说，给定函数的两点之间的近似值随着函数的二阶导数而变得越来越差。 这在直观上也是正确的：函数“越弯曲”，使用简单线性插值法得出的近似值就越差。

#### OpenCV

双线性内插值算法描述如下:  

对于一个目的像素，设置坐标通过反向变换得到的浮点坐标为`(i+u,j+v)` (其中 i、j 均为浮点坐标的整数部分，u、v 为浮点坐标的小数部分，是取值[0,1) 区间的浮点数)，则这个像素得值 `f(i+u,j+v) `可由原图像中坐标为`(i,j)`、`(i+1,j)`、`(i,j+1)`、`(i+1,j+1)`所对应的周围四个像素的值决定，即：`f(i+u,j+v) = (1-u)(1-v)f(i,j) + (1-u)vf(i,j+1) + u(1-v)f(i+1,j) + uvf(i+1,j+1)` 
其中 `f(i,j)`表示源图像`(i,j) `处的的像素值，以此类推。  

比如，现在假如目标图的象素坐标为（1，1），那么反推得到的对应于源图的坐标是（0.75 , 0.75）, 这其实只是一个概念上的虚拟象素, 实际在源图中并不存在这样一个象素, 那么目标图的象素（1，1）的取值不能够由这个虚拟象素来决定，而只能由源图的这四个象素共同决定：（0，0）（0，1）（1，0）（1，1），而由于（0.75,0.75）离（1，1）要更近一些，那么（1,1）所起的决定作用更大一些，这从公式 1 中的系数 uv=0.75×0.75 就可以体现出来，而（0.75,0.75）离（0，0）最远，所以（0，0）所起的决定作用就要小一些，公式中系数为 (1-u)(1-v)=0.25×0.25 也体现出了这一特点。

##### 计算方法

<img src="https://gitee.com/gaoyi-ai/image-bed/raw/master/images/image-20210302100515799.png" alt="image-20210302100515799" style="zoom:67%;" />  

首先，在 X 方向上进行两次线性插值计算，然后在 Y 方向上进行一次插值计算。   
$$
\begin{array}{l}
f\left(x, y_{1}\right) \approx \frac{x_{2}-x}{x_{2}-x_{1}} f\left(Q_{11}\right)+\frac{x-x_{1}}{x_{2}-x_{1}} f\left(Q_{21}\right) \\
f\left(x, y_{2}\right) \approx \frac{x_{2}-x}{x_{2}-x_{1}} f\left(Q_{12}\right)+\frac{x-x_{1}}{x_{2}-x_{1}} f\left(Q_{22}\right)
\end{array}
$$

$$
\begin{aligned}
f(x, y) & \approx \frac{y_{2}-y}{y_{2}-y_{1}} f\left(x, y_{1}\right)+\frac{y-y_{1}}{y_{2}-y_{1}} f\left(x, y_{2}\right) \\
& \approx \frac{y_{2}-y}{y_{2}-y_{1}}\left(\frac{x_{2}-x}{x_{2}-x_{1}} f\left(Q_{11}\right)+\frac{x-x_{1}}{x_{2}-x_{1}} f\left(Q_{21}\right)\right)+\frac{y-y_{1}}{y_{2}-y_{1}}\left(\frac{x_{2}-x}{x_{2}-x_{1}} f\left(Q_{12}\right)+\frac{x-x_{1}}{x_{2}-x_{1}} f\left(Q_{22}\right)\right) \\
&=\frac{1}{\left(x_{2}-x_{1}\right)\left(y_{2}-y_{1}\right)}\left(f\left(Q_{11}\right)\left(x_{2}-x\right)\left(y_{2}-y\right)+f\left(Q_{21}\right)\left(x-x_{1}\right)\left(y_{2}-y\right)+f\left(Q_{12}\right)\left(x_{2}-x\right)\left(y-y_{1}\right)+f\left(Q_{22}\right)\left(x-x_{1}\right)\left(y-y_{1}\right)\right)
\end{aligned}
$$

在图像处理的时候，我们先根据  `srcX=dstX* (srcWidth/dstWidth) `,`srcY = dstY * (srcHeight/dstHeight) ` 来计算目标像素在源图像中的位置，这里计算的 srcX 和 srcY 一般都是浮点数，比如 f（1.2, 3.4）这个像素点是虚拟存在的，先找到与它临近的四个实际存在的像素点  
（1，3） （2，3）  
（1，4） （2，4）  
写成 f(i+u,j+v) 的形式，则 u=0.2,v=0.4, i=1, j=3  
在沿着 X 方向差插值时，f(R1)=u(f(Q21)-f(Q11))+f(Q11)  
沿着 Y 方向同理计算。  
或者，直接整理一步计算，f(i+u,j+v) = (1-u)(1-v)f(i,j) + (1-u)vf(i,j+1) + u(1-v)f(i+1,j) + uvf(i+1,j+1) 

##### 加速以及优化策略

###### 源图像和目标图像几何中心的对齐 

方法：在计算源图像的虚拟浮点坐标的时候，一般情况：  
`srcX=dstX* (srcWidth/dstWidth) `  
`srcY = dstY * (srcHeight/dstHeight) ` 
中心对齐 (OpenCV 也是如此)：  
`SrcX=(dstX+0.5)* (srcWidth/dstWidth) -0.5`  
`SrcY=(dstY+0.5) * (srcHeight/dstHeight)-0.5`  

原理：

将公式变形，`srcX=dstX* (srcWidth/dstWidth)+0.5*(srcWidth/dstWidth-1) `相当于我们在原始的浮点坐标上加上了 `0.5*(srcWidth/dstWidth-1)` 这样一个控制因子，这项的符号可正可负，与 `srcWidth/dstWidth` 的比值也就是当前插值是扩大还是缩小图像有关，有什么作用呢？看一个例子：假设源图像是 3*3，中心点坐标（1，1）目标图像是 9*9，中心点坐标（4，4），我们在进行插值映射的时候，尽可能希望均匀的用到源图像的像素信息，最直观的就是（4,4）映射到（1,1）现在直接计算 srcX=4*3/9=1.3333！=1，也就是我们在插值的时候所利用的像素集中在图像的右下方，而不是均匀分布整个图像。现在考虑中心点对齐，srcX=(4+0.5)*3/9-0.5=1，刚好满足我们的要求。 

###### 将浮点运算转换成整数运算  

直接进行计算的话，由于计算的 srcX 和 srcY 都是浮点数，后续会进行大量的乘法，而图像数据量又大，速度不会理想，解决思路是：浮点运算→→整数运算→→”<<左右移按位运算”。  

放大的主要对象是 u，v 这些浮点数，OpenCV 选择的放大倍数是 2048，如何取这个合适的放大倍数呢，要从三个方面考虑，第一：精度问题，如果这个数取得过小，那么经过计算后可能会导致结果出现较大的误差。第二，这个数不能太大，太大会导致计算过程超过长整形所能表达的范围。第三：速度考虑。假如放大倍数取为 12，那么算式在最后的结果中应该需要除以 12*12=144，但是如果取为 16，则最后的除数为 16*16=256，这个数字好，我们可以用右移来实现，而右移要比普通的整除快多了。” 我们利用左移 11 位操作就可以达到放大目的。  

## Error

### EMA(exponential moving average)

**指数滑动平均法**是利用上一期的实际值和预测值 (估算值)，对它们进行不同的加权分配，求得一个指数平滑值，作为下一期预测值的一种预测方法。它的预测公式是：X~t~=α x S~t-1~+(1-α) x X~t-1~，(0<α<1)，式中，Xt 为第 t 期预测值，St-1 为上一期实际值；Xt-1 为上一期预测值；α为加权系数。指数平滑法是在移动平均法基础上发展起来的，它具有移动平均法的优点，又可以减少运算过程中的数据储存量，同时还考虑了不同时期的数据所起的不同作用。采用指数平滑法的关键是确定α值。一般情况下，α值的大小，既和反映近期数据的能力有关，也和数据波动状况有关。通常不直接利用一次指数平滑法来预测，而是利用二次指数平滑法，求出平滑系数，建立起预测模型，再进行预测。三次以上指数平滑法几乎适用于所有的时间序列预测，但在应用上有不少问题，所以实际上使用不多。

移动平均法]只是利用过去一部分序列来进行预测的，而且用的是算术平均值，即认为起作用的数据点对未来预测值起同等作用。这是不太合理的。为了弥补这些缺点，就产生了指数滑动平均法。指数滑动平均法是时间序列预测中的一种重要方法，也是一种简单而有效的预测手段。

指数滑动平均法是对整个时间序列进行加权平均的一种方法。加权平均就是每一个已知数据都对未来值贡献一部分力量。

已知一个时间序列$x_n,x_{n-1},...,x_t,x_{t-1},...,x_1$，它的一次加权平均值$x^{(1)}_t$为：
$$
x^{(1)}_t = a_0x_t + a_1x_{t-1} + a_2x_{t-2} +...
$$
其中$a_i$为加权系数。

若令$a_0=a$，通过数学论证，则有
$$
x^{(1)}_t = ax_t + (1-a)x^{(1)}_{t-1}
$$
其中：$x^{(1)}_t$为第 t 周期的一次指数平滑值，a为加权系数。

以一次指数平滑值为新的时间序列，再一次进行指数平滑，就可得二次指数平滑值$x^{(2)}_t$。
$$
x^{(2)}_t = ax_t + (1-a)x^{(2)}_{t-1}
$$

#### 指数滑动平均法加权系数

在平滑值公式 (1)，(2) 中，相当于：

平滑值 = a (新数据)+(1-a)(原平滑值)

因为原平滑值与旧数据有关，所以加权系数a是新旧数据在平滑中的分配比值。

取值的大小，实际上体现了不同时期的数据在预测中所起的不同作用。a越大，新数据所起的作用也就越大。若a过大，适应新水平过快，灵敏度高，容易对异常现象过敏，a过小，比较保守，容易落后于新的发展趋势。

### 优化算法之指数移动加权平均

#### 加权平均 VS 算术平均

*   算术平均数的定义：一般地，对于 n 个数 $x_{1},x_{2},x_{3},...,x_{n}$ 我们把$\frac{1}{n}(x_{1}+x_{2}+x_{3}+...+x_{n})$叫做这 n 个数的算术平均数
*   加权平均数：在实际问题中，一组数据里的各个数据的重要程度未必相同。因而，在计算这组数据的时候，往往给每个数据一个权。加权平均数一般来说，如果在 n 个数中， $x_{1}$出现的 $f_{1}$次，$x_{2}$  出现 $f_{2}$ 次，...， $x_{k}$ 出现$f_{k}$ 次，$(f_{1}+f_{2}+f_{3}+...+f_{k}=n)$，则$\bar{x}=\frac{1}{n}(x_{1}f_{1}+x_{2}f_{2}+x_{3}f_{3}+x_{k}f_{k})$ 其中 $f_{1},f_{2},f_{3}...f_{k}$叫做权。(权越大对平均数的影响也就越大)
*   算术平均数与加权平均数有什么区别? 算术平均数是加权平均数的一种特殊情况 (他特殊在各项的权相等为 1)；在实际问题中，各项权不相等的时，计算平均数时就要采用加权平均数，当各项权相等时，计算平均数就要采用算术平均数。
*   加权平均数中的权有 (1) 整数的形式；(2)比的形式；(3)百分比的形式；

**例子：**

整数的形式其实很好理解就是出现的频数。

其实这个例子的权重是股票占总股票的比重。也就是权重是一个比的形式。

<img src="https://gitee.com/gaoyi-ai/image-bed/raw/master/images/v2-10f45728e1296c85f4b7ee430336f5d8_r.jpg" style="zoom:67%;" /><img src="https://gitee.com/gaoyi-ai/image-bed/raw/master/images/v2-4285bdd0b6872235cce05d0037fed9f9_r.jpg" style="zoom:67%;" />

#### 加权平均法 VS 移动平均法

*   移动平均法是用一组最近的实际数据来预测未来一期或几期内的预测数一种常用方法。移动平均法适用于即期预测。当产品需求既不增长也不快速下降，且不存在在季节性因素时，移动平均法能有效地消除预测中的随机波动，是非常有用的。移动平均法根据预测时使用的各元素的权重不同，可以分为：简单移动平均 (一次移动平均法和二次移动平均法) 和加权移动平均。

##### 1. 简单的移动平均法

(一次移动平均法) 是收集一组观察值，计算这组观察值的均值，利用这个均值作为下一期的预测值。在移动平均值的计算中包括的过去观察值的实际个数，必须一开始就明确规定。每出现一个新的观察值，就要从移动平均中减去一个最早的观察值，再加上一个最新的观察值，计算移动平均值，这一新的移动平均值就最为下一期的预测值。

*   移动平均法有两种极端情况:(1) 在移动平均值的计算中包括的过去观察值的实际个数 N=1 ，这时利用最新的观察值作为下一期的预测值；(2) N=n ，这时利用全部的 n 个观察值的算术平均值作为预测值。
*   当数据的随机因素较大的时候，宜选用较大的 n ，这样有利于较大的限度地平滑由随机性所带来的严重偏差；反之，当数据的随机因素较小的时候，宜选用较小的 n ，这有利于跟踪数据的变化，并且预测值滞后的期数也少。
*   设时间序列 $x_{1},x_{2},x_{3},...,$ 移动平均法可以表示  $F_{t+1}=\frac{(x_{t}+x_{t-1}+x_{t-2}+...+x_{t-N+1})}{N}=\frac{1}{N}\sum_{t-N+1}^{t}{x_{i}}$式子中： $x_{t}$ 为最新观察值； $F_{t+1}$ 为下一期预测值；由移动平均法计算公式可以看出，每一新预测值是对前一移动平均预测值的修正， n越大平滑效果愈好。

*   移动平均法的优点：计算量少 ；移动平均线能较好的反应时间序列的趋势以及变化
*   移动平均法的两个主要限制：计算移动平均必须具有 n 个过去观察值，当需要预测大量的数值时，就必须存储大量数据； n个过去观察值中每一个权数都相等，而早于 $(t-N+1)$ 期的观察值的权数等于 0，而实际上往往是最新观察值包含更多信息，因具有更大的权重。

<img src="https://gitee.com/gaoyi-ai/image-bed/raw/master/images/v2-b1a30f1f11733f4248e4e31e6576a596_r.jpg" style="zoom:67%;" />

##### 2. 加权移动平均法

加权移动平均给固定跨越期限内的每个变量值以相等的权重。其原理是：历史各期产品需求的数据信息对预测未来期内的需求量的作用是不一样的。除了以 n 为周期性变化外，远离目标期的变量值的影响力相对较低，故应给予较低的权重。

*   加权移动平均法的计算公式： $F_{t}=w_{1}A_{t-1}+w_{2}A_{t-2}+w_{3}A_{t-3}+...+w_{n}A_{t-n}$ 其中式子中的 $w_{1}$ 是 t-1 期实际销售额的权重； $w_2$ 是第 t-2 期实际销售额的权重； $w_n$ 是第 y-n 期实际销售额的权重； n 为预测时期数； $w_{1}+w_{2}+w_{3}+...+w_{n}=1$。
*   在运用加权平均时，权重的选择是一个应该注意的问题，经验法和试算法使选择权重最简单的方法。一般而言，最近期的数据最能预测未来的情况。因而权重应大一些。例如，根据前一个月的利润和生产能力比起根据前几个月能更好的估计下一个月的利润和生产能力。但是，如果数据时季节性的，则权重也应该是季节性的。
*   使用移动平均法能预测能平滑掉需求的突然波动对预测结果的影响。但移动平均法运用时也存在着如下的问题：

*   加大移动平均法的期数 (就是 n ) 会使平滑波动效果更好，但会使预测值对数据实际变动更不敏感。(也就是图像会往右移动，有时延)
*   移动平均值并不能总是很好的反应出趋势。由于是平均值，预测值总是停留在过去的水平上而无法预计会导致将来更高或更低的波动。
*   移动平均法要大量的过去数据记录
*   使用越来越近的新数据，不断修改平均值。以之作为预测值。

*   移动平均法的基本原理：是通过移动平均消除时间序列中的不规则变动和其他变动，从而揭示出时间序列的长期趋势。

##### 补充：

指数平滑法是对加权移动平均法的改进，它是将前期预测值和前期实际值分别确定不同的权数 (二者权数和为 1)。只需要三个数据，所有预测方法中，指数平滑法采用较多，常用语短期预测。选择合适的 $\alpha$ 值。实际需求稳定，选取较小的 $\alpha$ 值，反之选取较大的 $\alpha$ 值。指数平滑法有很多种，有一次指数平滑预测、二次指数平滑预测以及三次指数平滑预测。我们这里这说一次指数平滑预测。

*   一次指数平滑预测是利用前一期的预测值 $F_{t}$ 代替 $x_{t-n}$ 得到预测的通式，即：$F_{t+1}=\alpha x_{t}+(1-\alpha)F_{t}$
*   由一次指数平滑法的通式可见：一次指数平滑法是一种加权预测，权数为 $\alpha$ 。它既不需要存储全部的历史数据，也不需要存储一组数据，从而可以大大减少数据存储问题，甚至有时只需一个最新观察值、最新预测值和 $\alpha$ 值，就可以进行预测。它提供的预测值是前一期预测值加上前期预测值中的误差的修正值。
*   一次指数平滑法的初始值的确定有几种方法：(1) 取第一期的实际值为初值；(2) 取最初几期的平均值为初值。一次指数平滑法比较简单，但也有问题。问题之一便是力图找到最佳的 $\alpha$ 值，以使均方差最小，这需要通过反复试验确定。

<img src="https://gitee.com/gaoyi-ai/image-bed/raw/master/images/v2-3901b51c6502b8605a020ca5a1b0b400_r.jpg" style="zoom: 50%;" />

<img src="https://gitee.com/gaoyi-ai/image-bed/raw/master/images/v2-559d18020b371aef4ce33ab5c0fcfa8d_r.jpg" style="zoom: 67%;" />

##### 指数加权移动平均

说了这么多那什么是指数加权移动平均呢？其实他也是加权移动平均的一种改进。指数加权移动平均 (Exponentially Weighted Moving Average)，他是一种常用的序列处理方式。在 t 时刻，他的移动平均值公式是： $V_{t}=\beta  V_{t-1}+(1-\beta)\theta_{t}$ $t=1,2,3,...n$ ，其中 $V_t$ 是 t 时刻的移动平均预测值； $\theta_{t}$ 为 t 时刻的真实值； $\beta$ 是权重；其实这个和上面的指数平滑预测很是相像。但是有有所不同，指数滑动平均 $F_{t+1}=\alpha+x_{t}+(1-\alpha)F_{t}$ 是通过当前 t 时间的真实值和 t 时间的预测值来进行估计预测下一个时期。而我们所说的指数加权移动平均就是通过当前的实际值和前一段时期 (由 $\beta$ 约定平均了多少以前的数据) 来进行平滑修改当前的值，来生成一个平稳的趋势曲线。

物理意义：系数 $\beta$ 越小就说明对过去测量值的权重越低，也就是对当前抽样值的权重越高。这个时候移动平均估计值的时效性就越强 (其实也就是更加拟合点分布的趋势)。反之，则会越弱。指数移动加权平均还有另一个特点就是能吸收瞬时突发的能力也就是平稳性 (使得得到的曲线趋势能够更加平缓)，如果对过去估计值的权重越低也就是 $\beta$ 越小，那么他的平稳性就差一点，反之平稳性会增强。

**举一个例子：**

![](https://gitee.com/gaoyi-ai/image-bed/raw/master/images/v2-54fa69a51cbe78d5f80929b1ab3fa078_r.jpg)

其实这里的曲线就是当 $v_{0}=0$ 作为移动平均的初始值，然后将对应的实际的温度值带入递归式子中，然后得出的曲线。其实从上面也可以看出来 $\beta$ 的选择尤为的重要。这个温度的例子，吴恩达老师选择了 0.9 作为 $\beta$ 的值。可以看出曲线要平坦一点，这是因为你平均了几天的温度，所以这个曲线波动更小，更加平坦，缺点就是曲线会失去时效性，在图中的表现就是曲线会向右移动，那因为现在要平均的温度值更多，要平均更多的值，指数加权平均公式在温度变化的时，能更加适应缓慢一些，所以会出现一定的延迟。

![](https://gitee.com/gaoyi-ai/image-bed/raw/master/images/v2-aa518c5987027637d0a203970a54e6fa_r.jpg)

*   $\beta=0.98$ 的时候，曲线会更加平缓 (稳定性高)，但是趋势曲线会向右移动 (时效性差)
*   $\beta=0.5$ 的时候，由于只平均了两天的温度，平均的数据太少了，得到的曲线会有更多的噪声也就是 (稳定性差) 更有可能出现异常值，但是趋势曲线能够更加适应拟合你的原始数据，也就是趋势曲线的(时效性高)

通过上面两个极端值可以看出，我们可以选择一个合适的 $\beta$ 值来使曲线既平缓又不偏离数据点。也就是不会有太多的噪声同时也不会向右偏离太多。 $\beta$ 是一个很重要的参数，可以取得稍微不同的效果，往往中间某个值效果最好。那我们说为什么知道 $\beta$ 值就知道他平均了多少天呢？

3. 指数移动加权平均的理解

--------------

我们使用 $$\beta=0.98$$ 来看看指数移动加权平均的原理是什么?

$V_{100}=0.9V_{99}+0.1\theta_{100}$

$V_{99}=0.9V_{98}+0.1\theta_{99}$

$V_{98}=0.9V_{97}+0.1\theta_{98}$

...

我们将式子一步一步的带入得到最终式子： $V_{100}=0.1\theta_{100}+0.1*0.9\theta_{99}+0.1*0.9*0.9\theta_{98}+...+0.1*0.9^{99}\theta_{1}$ 通过式子我们可以很清楚的看出对于求的 $V_{100}$ 的值，可以看做是 $\theta_{1}\rightarrow\theta_{100}$ 的温度值与对应的指数衰减函数对应项相乘之后在求和。

![](https://gitee.com/gaoyi-ai/image-bed/raw/master/images/v2-f640343121f816a2dd32a65fca3074b4_b-1614753075881-1614753082796-1614753135298.jpg)

我们上回看到当 $\beta=0.5$ 的时候我们说平均了两天，我们发现  $0.5^{2}\approx\frac{1}{e}\approx+0.35$再去乘于 0.5 那么可以看出后面的数也会很小，所以我们再去考虑。同理， $\beta=0.9$的时候，也就说$0.9^{10}\approx+\frac{1}{e}\approx0.35$平均了 10 天。 $\frac{1}{e}*0.1$ 我们认为这个值就已经很小了，所以不再去考虑后面的数据了，所以说当 $\beta=0.9$ 的时候只平均了 10 天的数据。

**优势：**

$V$ 是用来计算数据的指数加权平均数，计算指数加权平均数只占单行数字的存储和内存，

当然并不是最好的，也不是最精准的计算平均数的方法，如果你需要计算时间窗，你可以直

接过去 10 天的总和或者过去 50 天的总和除以 10 或 50 就好了，如此往往会得到更好的估

测，但缺点是如果保存最近的气温和过去 10 天的总和，必须占更多的内存，执行更加复杂，

而计算指数加权平均数只占单行数字的存储和内存。他的效率和资源的占有率会大大的减小。 所以在机器学习中大部分采用指数加权平均的方法计算平均值。

4. 指数加权移动平均的偏差修正

----------------

当我们取 $\beta=0.98$ 的时候，实际上我们得到的并不是绿色的曲线而是紫色的曲线，通过紫色曲线我们看出在预测的初期值和我的真实值的差距很大，所以引入了偏差修正的概念。

<img src="https://gitee.com/gaoyi-ai/image-bed/raw/master/images/v2-707771284e94a179256dfc63642c3709_r-1614753025596-1614753098177.jpg" style="zoom:80%;" />

*   还是上面温度的例子我们 $V_{0}=0$ ， $\theta_{1}=40$ 那么通过指数移动加权平均的公式可以得到 $V_{1}=0.98V_{0}+0.02\theta_{1}=0.98*0+0.02*40=8$ 那可以看出算出的 $V_{1}=8$ 和实际 ![](https://www.zhihu.com/equation?tex=%5Ctheta_%7B1%7D) 的 ![](https://www.zhihu.com/equation?tex=40) 相比差距还是不小的。 $V_{2}=0.98V_{1}+0.02\theta_{2}=0.0196*\theta_{1}+0.02*\theta_{2}$ 同理 $V_2$ 也要远远小于 1 号和 2 号数据。所以可以看出  $V_1$... 这样的前期移动平均值并不能很好的估测温度。
*   引入偏差就是为了解决估测初期预测不准确的问题。那么如何去做呢？

*   指数加权平均公式： $V_{t}=\beta+V_{t-1}+(1-\beta)\theta_{t}$
*   带修正偏差的指数加权平均公式： $V_{t}^{'}=\frac{V_{t}}{1-\beta^{t}}=\frac{(\beta+V_{t-1}+(1-\beta)\theta_t)}{1-\beta^{t}}$

*   当 t=2 的时候， $1-\beta^{2}=1-(0.98)^2=0.0396$ , $V_{2}^{'}=\frac{V_{2}}{1-\beta^{2}}=\frac{0.0196*\theta_{1}+0.02*\theta_{2}}{0.0396}$ ，可以看出比原来的效果好了很多。对于 $1-\beta^t$ 我们可以看出，随着 t 的逐渐增大， $\beta^t$ 会逐渐接近与 0，那么 $1-\beta^t$ 就会逐渐接近与 1 ，那么我们从公式上可以看出，我们的偏差修正最终会变成 (如果数据多的话)  $V^{'}_{t}=V_{t}$，公式最终会变成$V^{'}_{t}=V_{t}=\beta+V_{t-1}+(1-\beta)\theta_{t}$。所以在机器学习中，在计算指数加权平均数的大部分时候，大家不太在乎偏差修正，大部分宁愿熬过初始阶段，拿到具有偏差的估测，然后继续计算下去。如果你关心初始时期的偏差，修正偏差能帮助你在早期获得更好的估测。

其实滑动平均模型的原理就是一阶滞后滤波法，其表达式如下： 

`new_value=(1−weight)×value+weight×old_value`

其中weight的取值范围[0,1]，具体就是：本次滤波结果=(1-weight)本次采样值+weight上次滤波结果，采用此算法的目的是： 


1、降低周期性的干扰； 

2、在波动频率较高的场景有很好的效果。
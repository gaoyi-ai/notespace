---
title: Jacobian & Hessian
categories:
- Optimization
tags:
- Jacobian 
- Hessian
date: 2021/3/7 10:00:00
updated: 2021/3/7 16:00:00
---

# Jacobian 矩阵和 Hessian 矩阵

## 雅可比矩阵

雅可比矩阵的重要性在于它体现了一个可微方程与给出点的最优线性逼近. 因此, 雅可比矩阵类似于多元函数的导数.

假设 $F$: $Rn→Rm$是一个从欧式 n 维空间转换到欧式 m 维空间的函数. 这个函数由 m 个实函数组成: y1(x1,…,xn), …, ym(x1,…,xn). 这些函数的偏导数 (如果存在) 可以组成一个 m 行 n 列的矩阵

此矩阵表示为: $J_F(x1,…,xn)$, 或者$\frac{∂(y1,…,ym)}{∂(x1,…,xn)}$.

这个矩阵的第 i 行是由梯度函数的转置 yi(i=1,…,m) 表示的.

如果 $p$是 $Rn$中的一点, $F$在 $p$点可微分, 那么在这一点的导数由 $J_F(p)$给出 (这是求该点导数最简便的方法). 在此情况下, 由 $F(p)$描述的线性算子即接近点 $p$的 $F$的最优线性逼近, $x$逼近于 $p$:

$F(x)≈F(p)+J_F(p)⋅(x–p)$

### 雅可比行列式

如果 m = n, 那么 $F$是从 n 维空间到 n 维空间的函数, 且它的雅可比矩阵是一个方块矩阵. 于是我们可以取它的行列式, 称为雅可比行列式.

在某个给定点的雅可比行列式提供了 在接近该点时的表现的重要信息. 例如, 如果连续可微函数 $F$在 $p$点的雅可比行列式不是零, 那么它在该点附近具有反函数. 这称为反函数定理. 

更进一步, 如果 $p$点的雅可比行列式是正数, 则 $F$在 $p$点的取向不变；如果是负数, 则 $F$的取向相反. 而从雅可比行列式的绝对值, 就可以知道函数 $F$在 $p$点的缩放因子；这就是为什么它出现在换元积分法中.

对于取向问题可以这么理解, 例如一个物体在平面上匀速运动, 如果施加一个正方向的力 $F$, 即取向相同, 则加速运动, 类比于速度的导数加速度为正；如果施加一个反方向的力 $F$, 即取向相反, 则减速运动, 类比于速度的导数加速度为负.

## Hessian 矩阵

Hessian 矩阵被应用于牛顿法解决的大规模优化问题.

># Hessian Matrix
>
>## Definitions and properties
>
>在数学中，Hessian matrix是一个标量值函数或标量场的二阶偏导数的平方矩阵。它描述的是一个多变量函数的局部曲率。
>
>- Hessian矩阵是一个对称矩阵，因为二次导数的连续性假设意味着微分的顺序并不重要（Schwarz定理）。
>
>- Hessian矩阵的行列式称为Hessian行列式。
>
>- 函数f的Hessian矩阵是函数f的梯度的雅各布矩阵，也就是说。H(f(x)) = J(∇f(x))。
>
>## Applications
>
>### 拐点
>
>如果f是三变量中的同次多项式，则方程f=0是平面投影曲线的隐式方程。曲线的拐点恰好是赫斯行列式为零的非星点。由Bézout定理可知，立方平面曲线最多只有9个拐点，因为黑森行列式是3度的多项式。
>
>### 二次导数检验
>
>*Main article:* [Second partial derivative test](https://en.wikipedia.org/wiki/Second_partial_derivative_test)
>
>凸函数的Hessian矩阵是正半定义的。完善这一性质，我们可以检验临界点x是局部最大值、局部最小值还是鞍点，具体如下。
>
>如果Hessian在x处为正定义，那么f在x处达到一个孤立的局部最小值。 如果Hessian在x处为负定义，那么f在x处达到一个孤立的局部最大值。 如果Hessian有正和负的特征值，那么x是f的一个鞍点，否则测试是不确定的。这意味着，在局部最小值处，赫斯是正半定义，而在局部最大值处，赫斯是负半定义。
>
>需要注意的是，对于正半定义和负半定义的黑森，检验是不确定的（黑森为半定义但不确定的临界点可能是一个局部极点或鞍点）。然而，从Morse理论的角度可以说得更多。
>
>一变量和二变量函数的二乘检验很简单。在一个变量中，Hessian只包含一个二次导数，如果是正数，则x是局部最小值，如果是负数，则x是局部最大值；如果是零，则检验没有结论。在两个变量中，可以用行列式，因为行列式是特征值的乘积。如果是正值，那么特征值都是正值，或者都是负值。如果是负值，那么两个特征值的符号不同。如果是零，那么二阶检验就没有结论。
>
>同理，局部最小或最大的二阶充分条件可以用Hessian矩阵的主（最左上）小（子矩阵的行列式）序列来表示；这些条件是下一节中给出的用于约束优化的bordered Hessians的特殊情况--约束数为零的情况。具体来说，最小值的充分条件是所有这些主小数都是正数，而最大值的充分条件是小数的符号交替，1×1小数为负数。
>
>### 关键点
>
>如果函数f的梯度(偏导数的向量)在某点x处为零，那么f在x处有一个临界点(或静止点)。在x处的Hessian 的行列式在某些情况下称为判据。如果这个行列式为零，那么x称为f的退化临界点，或f的非Morse临界点，否则为非退化，称为f的Morse临界点。
>
>Hessian矩阵在Morse理论和灾难理论中起着重要的作用，因为它的核和特征值可以对临界点进行分类[2][3][4] 。
>
>当评价一个函数的临界点时，Hessian矩阵的行列式等于被视为歧面的函数的高斯曲率。该点的Hessian 的特征值是函数的原理曲率，特征向量是曲率的原理方向。见高斯曲率§与主曲率的关系）。
>
>### 用于优化
>
>Hessian矩阵在牛顿型方法中用于大规模优化问题，因为它是函数局部泰勒展开的二次项的系数。即：
>$$
>{\displaystyle y=f(\mathbf {x} +\Delta \mathbf {x} )\approx f(\mathbf {x} )+\nabla f(\mathbf {x} )\Delta \mathbf {x} +{\frac {1}{2}}\,\Delta \mathbf {x} ^{\mathrm {T} }\mathbf {H} (\mathbf {x} )\,\Delta \mathbf {x} }
>$$
>其中∇f为梯度$(∂f/∂x1, ..., ∂f/∂xn)$。计算和存储完整的Hessian 矩阵需要Θ(n2)内存，这对于高维函数来说是不可行的，如神经网的损失函数、条件随机场和其他具有大量参数的统计模型。针对这种情况，人们开发了截断-牛顿和准牛顿算法。后一种算法系列使用对Hessian 的近似；最流行的准牛顿算法之一是BFGS[5]。
>
>这样的近似可以利用优化算法只使用Hessian作为线性算子H(v)的事实，并通过首先注意到Hessian也出现在梯度的局部展开中来进行。
>$$
>{\displaystyle \nabla f(\mathbf {x} +\Delta \mathbf {x} )=\nabla f(\mathbf {x} )+\mathbf {H} (\mathbf {x} )\,\Delta \mathbf {x} +{\mathcal {O}}(\|\Delta \mathbf {x} \|^{2})}
>$$
>对于一些标量r，让Δx=rv，这就可以得到
>$$
>{\displaystyle \mathbf {H} (\mathbf {x} )\,\Delta \mathbf {x} =\mathbf {H} (\mathbf {x} )r\mathbf {v} =r\mathbf {H} (\mathbf {x} )\mathbf {v} =\nabla f(\mathbf {x} +r\mathbf {v} )-\nabla f(\mathbf {x} )+{\mathcal {O}}(r^2)}
>$$
>比如，
>$$
>{\displaystyle \mathbf {H} (\mathbf {x} )\mathbf {v} ={\frac {1}{r}}{\Bigl [}\nabla f(\mathbf {x} +r\mathbf {v} )-\nabla f(\mathbf {x} ){\Bigr ]}+{\mathcal {O}}(r)}
>$$
>因此，如果梯度已经计算出来了，那么近似的Hessian 可以通过线性（以梯度的大小）的标量运算来计算。(虽然编程简单，但这种近似方案在数值上并不稳定，因为r必须做得很小，以防止由于${\displaystyle {\mathcal {O}}(r)}$项引起的错误，但减少它就会失去第一项的精度。[6])
---
title: Jacobian & Hessian
categories:
- Optimization
tags:
- Jacobian 
- Hessian
date: 2021/3/7 10:00:00
updated: 2021/3/7 16:00:00
---

# Jacobian 矩阵和 Hessian 矩阵

## 雅可比矩阵

雅可比矩阵的重要性在于它体现了一个可微方程与给出点的最优线性逼近. 因此, 雅可比矩阵类似于多元函数的导数.

假设 $F$: $Rn→Rm$是一个从欧式 n 维空间转换到欧式 m 维空间的函数. 这个函数由 m 个实函数组成: y1(x1,…,xn), …, ym(x1,…,xn). 这些函数的偏导数 (如果存在) 可以组成一个 m 行 n 列的矩阵

此矩阵表示为: $J_F(x1,…,xn)$, 或者$\frac{∂(y1,…,ym)}{∂(x1,…,xn)}$.

这个矩阵的第 i 行是由梯度函数的转置 yi(i=1,…,m) 表示的.

如果 $p$是 $Rn$中的一点, $F$在 $p$点可微分, 那么在这一点的导数由 $J_F(p)$给出 (这是求该点导数最简便的方法). 在此情况下, 由 $F(p)$描述的线性算子即接近点 $p$的 $F$的最优线性逼近, $x$逼近于 $p$:

$F(x)≈F(p)+J_F(p)⋅(x–p)$

### 雅可比行列式

如果 m = n, 那么 $F$是从 n 维空间到 n 维空间的函数, 且它的雅可比矩阵是一个方块矩阵. 于是我们可以取它的行列式, 称为雅可比行列式.

在某个给定点的雅可比行列式提供了 在接近该点时的表现的重要信息. 例如, 如果连续可微函数 $F$在 $p$点的雅可比行列式不是零, 那么它在该点附近具有反函数. 这称为反函数定理. 

更进一步, 如果 $p$点的雅可比行列式是正数, 则 $F$在 $p$点的取向不变；如果是负数, 则 $F$的取向相反. 而从雅可比行列式的绝对值, 就可以知道函数 $F$在 $p$点的缩放因子；这就是为什么它出现在换元积分法中.

对于取向问题可以这么理解, 例如一个物体在平面上匀速运动, 如果施加一个正方向的力 $F$, 即取向相同, 则加速运动, 类比于速度的导数加速度为正；如果施加一个反方向的力 $F$, 即取向相反, 则减速运动, 类比于速度的导数加速度为负.

## 海森 Hessian 矩阵

海森矩阵被应用于牛顿法解决的大规模优化问题.

### 海森矩阵在牛顿法中的应用

一般来说, 牛顿法主要应用在两个方面, 1, 求方程的根; 2, 最优化.

1. 求解方程

并不是所有的方程都有求根公式, 或者求根公式很复杂, 导致求解困难. 利用牛顿法, 可以迭代求解.

原理是利用泰勒公式, 在 $x_0$处展开, 且展开到一阶, 即 $f(x)=f(x_0)+(x–x_0)f′(x0)$

求解方程 $f(x)=0$, 即 $f(x_0)+(x–x_0)f′(x_0)=0$, 求解 $x=x_1=x_0–f(x_0)/f′(x_0)$, 因为这是利用泰勒公式的一阶展开, $f(x)=f(x_0)+(x–x_0)f′(x_0)$处并不是完全相等, 而是近似相等, 这里求得的 $x_1$并不能让 $f(x)=0$, 只能说 $f(x_1)$的值比 $f(x_0)$更接近 $f(x)=0$, 于是乎, 迭代求解的想法就很自然了, 可以进而推出 $x_{n+1}=x_n–f(x_n)/f′(x_n)$, 通过迭代, 这个式子必然在 $f(x^∗)=0$的时候收敛. 整个过程如下图：

![](https://gitee.com/gaoyi-ai/image-bed/raw/master/images/697b070fjw1dvpdvfz24hj.jpg)

2. 最优化

在最优化的问题中, 线性最优化至少可以使用单纯形法 (或称不动点算法) 求解, 但对于非线性优化问题, 牛顿法提供了一种求解的办法. 假设任务是优化一个目标函数 $f$, 求函数 $f$的极大极小问题, 可以转化为求解函数 $f$的导数 $f′=0$的问题, 这样求可以把优化问题看成方程求解问题 ($f′=0$). 剩下的问题就和第一部分提到的牛顿法求解很相似了.

这次为了求解 $f′=0$的根, 首先把 $f(x)$在探索点 $x_n$处泰勒展开, 展开到 2 阶形式进行近似：

$$f(x)=f(x_n)+f′(x_n)(x–x_n)+\frac{f”(x_n)}{2}(x–x_n)^2$$

然后用 $f(x)$的最小点做为新的探索点 $x_{n+1}$，据此，令：

$$f′(x)=f′(x_n)+f”(x_n)(x–x_n)=0$$

求得出迭代公式：

$$x_{n+1}=x_n–f′(x_n)f”(x_n),n=0,1,…$$

一般认为牛顿法可以利用到曲线本身的信息, 比梯度下降法更容易收敛（迭代更少次数）, 如下图是一个最小化一个目标方程的例子, 红色曲线是利用牛顿法迭代求解, 绿色曲线是利用梯度下降法求解.

![](https://gitee.com/gaoyi-ai/image-bed/raw/master/images/20170616223638108)

在上面讨论的是 2 维情况, 高维情况的牛顿迭代公式是：

$$x_{n+1}=x_n–[Hf(x_n)]–1∇f(x_n),n≥0$$

其中 H 是 Hessian 矩阵, 定义见上. 

高维情况依然可以用牛顿迭代求解, 但是问题是 Hessian 矩阵引入的复杂性, 使得牛顿迭代求解的难度大大增加, 但是已经有了解决这个问题的办法就是 Quasi-Newton method, 不再直接计算 hessian 矩阵, 而是每一步的时候使用梯度向量更新 hessian 矩阵的近似.

># Hessian Matrix
>
>## Definitions and properties
>
>在数学中，Hessian matrix是一个标量值函数或标量场的二阶偏导数的平方矩阵。它描述的是一个多变量函数的局部曲率。
>
>- Hessian矩阵是一个对称矩阵，因为二次导数的连续性假设意味着微分的顺序并不重要（Schwarz定理）。
>
>- Hessian矩阵的行列式称为Hessian行列式。
>
>- 函数f的Hessian矩阵是函数f的梯度的雅各布矩阵，也就是说。H(f(x)) = J(∇f(x))。
>
>## Applications
>
>### 拐点
>
>如果f是三变量中的同次多项式，则方程f=0是平面投影曲线的隐式方程。曲线的拐点恰好是赫斯行列式为零的非星点。由Bézout定理可知，立方平面曲线最多只有9个拐点，因为黑森行列式是3度的多项式。
>
>### 二次导数检验
>
>*Main article:* [Second partial derivative test](https://en.wikipedia.org/wiki/Second_partial_derivative_test)
>
>凸函数的Hessian矩阵是正半定义的。完善这一性质，我们可以检验临界点x是局部最大值、局部最小值还是鞍点，具体如下。
>
>如果Hessian在x处为正定义，那么f在x处达到一个孤立的局部最小值。 如果Hessian在x处为负定义，那么f在x处达到一个孤立的局部最大值。 如果Hessian有正和负的特征值，那么x是f的一个鞍点，否则测试是不确定的。这意味着，在局部最小值处，赫斯是正半定义，而在局部最大值处，赫斯是负半定义。
>
>需要注意的是，对于正半定义和负半定义的黑森，检验是不确定的（黑森为半定义但不确定的临界点可能是一个局部极点或鞍点）。然而，从Morse理论的角度可以说得更多。
>
>一变量和二变量函数的二乘检验很简单。在一个变量中，Hessian只包含一个二次导数，如果是正数，则x是局部最小值，如果是负数，则x是局部最大值；如果是零，则检验没有结论。在两个变量中，可以用行列式，因为行列式是特征值的乘积。如果是正值，那么特征值都是正值，或者都是负值。如果是负值，那么两个特征值的符号不同。如果是零，那么二阶检验就没有结论。
>
>同理，局部最小或最大的二阶充分条件可以用Hessian矩阵的主（最左上）小（子矩阵的行列式）序列来表示；这些条件是下一节中给出的用于约束优化的bordered Hessians的特殊情况--约束数为零的情况。具体来说，最小值的充分条件是所有这些主小数都是正数，而最大值的充分条件是小数的符号交替，1×1小数为负数。
>
>### 关键点
>
>如果函数f的梯度(偏导数的向量)在某点x处为零，那么f在x处有一个临界点(或静止点)。在x处的Hessian 的行列式在某些情况下称为判据。如果这个行列式为零，那么x称为f的退化临界点，或f的非Morse临界点，否则为非退化，称为f的Morse临界点。
>
>Hessian矩阵在Morse理论和灾难理论中起着重要的作用，因为它的核和特征值可以对临界点进行分类[2][3][4] 。
>
>当评价一个函数的临界点时，Hessian矩阵的行列式等于被视为歧面的函数的高斯曲率。该点的Hessian 的特征值是函数的原理曲率，特征向量是曲率的原理方向。见高斯曲率§与主曲率的关系）。
>
>### 用于优化
>
>Hessian矩阵在牛顿型方法中用于大规模优化问题，因为它是函数局部泰勒展开的二次项的系数。即：
>$$
>{\displaystyle y=f(\mathbf {x} +\Delta \mathbf {x} )\approx f(\mathbf {x} )+\nabla f(\mathbf {x} )\Delta \mathbf {x} +{\frac {1}{2}}\,\Delta \mathbf {x} ^{\mathrm {T} }\mathbf {H} (\mathbf {x} )\,\Delta \mathbf {x} }
>$$
>其中∇f为梯度$(∂f/∂x1, ..., ∂f/∂xn)$。计算和存储完整的Hessian 矩阵需要Θ(n2)内存，这对于高维函数来说是不可行的，如神经网的损失函数、条件随机场和其他具有大量参数的统计模型。针对这种情况，人们开发了截断-牛顿和准牛顿算法。后一种算法系列使用对Hessian 的近似；最流行的准牛顿算法之一是BFGS[5]。
>
>这样的近似可以利用优化算法只使用Hessian作为线性算子H(v)的事实，并通过首先注意到Hessian也出现在梯度的局部展开中来进行。
>$$
>{\displaystyle \nabla f(\mathbf {x} +\Delta \mathbf {x} )=\nabla f(\mathbf {x} )+\mathbf {H} (\mathbf {x} )\,\Delta \mathbf {x} +{\mathcal {O}}(\|\Delta \mathbf {x} \|^{2})}
>$$
>对于一些标量r，让Δx=rv，这就可以得到
>$$
>{\displaystyle \mathbf {H} (\mathbf {x} )\,\Delta \mathbf {x} =\mathbf {H} (\mathbf {x} )r\mathbf {v} =r\mathbf {H} (\mathbf {x} )\mathbf {v} =\nabla f(\mathbf {x} +r\mathbf {v} )-\nabla f(\mathbf {x} )+{\mathcal {O}}(r^2)}
>$$
>比如，
>$$
>{\displaystyle \mathbf {H} (\mathbf {x} )\mathbf {v} ={\frac {1}{r}}{\Bigl [}\nabla f(\mathbf {x} +r\mathbf {v} )-\nabla f(\mathbf {x} ){\Bigr ]}+{\mathcal {O}}(r)}
>$$
>因此，如果梯度已经计算出来了，那么近似的Hessian 可以通过线性（以梯度的大小）的标量运算来计算。(虽然编程简单，但这种近似方案在数值上并不稳定，因为r必须做得很小，以防止由于{\displaystyle {\mathcal {O}}(r)}{\displaystyle {\mathcal {O}}(r)}项引起的错误，但减少它就会失去第一项的精度。[6])
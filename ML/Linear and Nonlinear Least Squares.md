---
title: Linear and Nonlinear Least Squares
categories:
- Optimization
tags:
- gradient descent
- Newton 
date: 2021/3/10 20:00:17
updated: 2021/3/12 12:00:17
---

# Least squares

最小二乘法是回归分析中的一种标准方法，通过最小化每一个方程结果中所做的残差的平方和，来近似求解超定系统（方程多于未知数的方程组）。

>在数学上，线性最小二乘是近似求解线性方程组A x = b的超定系统的问题，其中b不是矩阵A的列空间的元素。近似解被实现为A x =的精确解 b'，其中b'是b在A的列空间上的投影。因此，最佳近似是使数据值与其对应的建模值之间的平方差之和最小的方法。 该方法称为线性最小二乘，因为假定函数在要估计的参数中是线性的。 线性最小二乘问题是凸的，并且具有封闭式解，该解是唯一的，条件是用于拟合的数据点的数量等于或超过未知参数的数量，除非在特殊的退化情况下。 相反，非线性最小二乘问题通常必须通过迭代过程来解决，并且这些问题可以是具有目标函数的多个最优值的非凸问题。 如果先前的分布可用，那么甚至可以使用贝叶斯MMSE估计器求解不确定的系统。

最重要的应用是在数据拟合中。最小二乘法意义上的最佳拟合能使平方残差之和最小化（残差是指：观测值，与模型提供的拟合值之间的差异）。当问题中的自变量（x变量）具有很大的不确定性时，那么简单的回归法和最小二乘法就会出现问题；在这种情况下，可以考虑用变量误差模型拟合所需的方法来代替最小二乘法。

最小二乘法问题可分为两类：线性或普通最小二乘法和非线性最小二乘法，取决于残差是否在所有未知数中都是线性的。线性最小二乘问题出现在统计回归分析中，它有一个闭式解。非线性问题通常通过迭代细化来解决；在每一次迭代中，系统都会被线性系统所近似，因此两种情况下的核心计算是相似的。

## Problem statement

目标包括调整模型函数的参数，使其最适合数据集。一个简单的数据集由n个点(数据对)$(x_{i},y_{i})$，i=1，...，n，其中$x_i$是一个独立变量，{/displaystyle y_{i}/!}y_{i}/!}是一个因变量，其值由观察发现。模型函数的形式为$f(x, \beta)$，其中m个可调节参数存放在向量$\boldsymbol \beta $中。目标是找到 "最 "适合数据的模型的参数值。模型对数据点的拟合度用残差来衡量，残差定义为因变量的实际值与模型预测值之间的差异：$r_{i}=y_{i}-f(x_{i},{\boldsymbol  \beta })$

最小二乘法通过最小化平方残差的和，$S$，找到最佳参数值。$ S=\sum _{i=1}^{n}r_{i}^{2}.$

二维模型的一个例子是直线。将y截距表示为$\beta_0$，斜率表示为$\beta_1$。该模型函数由$ f(x, \boldsymbol \beta )=\beta _{0}+\beta _{1}x$给出。

一个数据点可能由一个以上的独立变量组成。例如，当将一个平面拟合到一组身高测量值上时，这个平面是两个独立变量x和z的函数，比如说。在最一般的情况下，每个数据点可能有一个或多个独立变量和一个或多个因变量。

右边是一个残差图，显示了关于$r_i=0$的随机波动，表明线性模型$ (Y_{i}=\alpha +\beta x_{i}+U_{i})$是合适的。$U_i$是一个独立的随机变量。 

如果残差点有某种形状，而且不是随机波动的，那么线性模型就不合适。例如，如果残差图具有抛物线形状，如右图所示，抛物线模型$(Y_{i}=\alpha +\beta x_{i}+\gamma x_{i}^{2}+U_{i})$将适用于该数据。抛物线模型的残差可以通过$ r_{i}=y_{i}-{\hat {\alpha }}-{\hat {\beta }}x_{i}-\widehat {\gamma }x_{i}^{2}$来计算。

## Limitations

这种回归公式只考虑了因变量的观测误差(但另一种总的最小二乘回归可以考虑两个变量的误差)。有两种相当不同的背景，有着不同的意义。

用于预测的回归。这里拟合模型是为了提供一个预测规则，以便在类似的情况下应用，而用于拟合的数据也适用于这种情况。在这里，对应于这种未来应用的因变量将受到与用于拟合的数据中的观察误差相同类型的影响。因此，对这类数据使用最小二乘预测规则在逻辑上是一致的。

拟合 "真实关系 "的回归。在用最小二乘法进行拟合的标准回归分析中，有一个隐含的假设，即自变量的误差为零或严格控制，使其可以忽略不计。当独立变量中的误差不可忽略时，可以使用测量误差模型；这种方法可以导致参数估计、假设检验和置信区间，考虑到独立变量中观察误差的存在[11]。 另一种方法是用完全最小二乘法拟合模型；这可以看作是在制定用于模型拟合的目标函数时，采取一种务实的方法来平衡不同误差源的影响。

## Differences between linear and nonlinear least squares

在LLSQ（线性最小二乘法）中，模型函数f是参数的线性组合，其形式为$f=X_{i1} \beta _{1}+X_{i2} \beta _{2}+\cdots $该模型可以表示一条直线、一条抛物线或任何其他函数的线性组合。在NLLSQ（非线性最小二乘法）中，参数以函数的形式出现，如$\beta ^{2}$,$e^{\beta x}$等。如果导数$\partial  f/\ \partial \beta _{j}$要么是常数，要么只取决于自变量的值，则模型的参数是线性的。否则模型是非线性的。
需要参数的初始值才能找到NLLSQ问题的解，LLSQ不需要初始值。

NLLSQ的求解算法往往要求能计算出类似LLSQ的雅各布值。部分导数的分析表达式可能比较复杂。如果无法获得分析表达式，则必须通过数值近似计算部分导数，或者通过有限差分对雅各布进行估计。

在NLLSQ中，不收敛（算法找不到最小值）是一种常见现象。

LLSQ是全局凹型的，所以不收敛不是问题。

解NLLSQ通常是一个迭代过程，当满足收敛标准时，必须终止。LLSQ的解可以用直接的方法计算，尽管有大量参数的问题通常用迭代方法解决，如Gauss-Seidel方法。

在LLSQ中，解是唯一的，但在NLLSQ中，可能存在多个平方和的最小值。

在误差与预测变量不相关的条件下，LLSQ可以得到无偏估计，但即使在这个条件下，NLLSQ的估计通常也是有偏的。

每当寻求非线性最小二乘法问题的解决方案时，都必须考虑这些差异[12] 。

## Regression analysis and statistics

在回归分析中，最小二乘法经常被用来生成估计器和其他统计量。

考虑一个来自物理学的简单例子。一个弹簧应该服从胡克定律，该定律指出弹簧y的延伸与施加在它身上的力F成正比。$y=f(F,k)=kF$
构成模型，其中F为自变量。为了估计力常数k，我们用不同的力进行一系列n次测量，产生一组数据，$(F_{i},y_{i}),i=1,\dots ,n$，其中yi是测量的弹簧延伸量.[14]每个实验观测值都会包含一些误差，$\varepsilon$ ，因此我们可以为我们的观测值指定一个经验模型。$ y_{i}=kF_{i}+\varepsilon _{i}$
由于我们的数据中m个变量中的n个方程由一个未知数和n个方程组成的超前系统组成，我们使用最小二乘法来估计k。需要最小化的平方和为$ S=\sum _{i=1}^{n}(y_{i}-kF_{i})^{2}$

力常数k的最小二乘法估计值由以下公式给出。$\hat{k}=\frac {\sum _{i}F_{i}y_{i}}{\sum _{i}F_{i}^{2}}$
我们假设施力会导致弹簧膨胀。通过最小二乘法拟合得出力常数后，我们根据胡克定律预测延伸。

我们假设施加力会导致弹簧膨胀。在通过最小二乘法拟合得出力常数后，我们根据胡克定律预测延伸。

研究者在回归分析中指定一个经验模型。一个非常常见的模型是直线模型，它用于检验独立变量和因变量之间是否存在线性关系。如果存在线性关系，则称变量之间存在相关性。但是，相关性并不能证明因果关系，因为这两个变量可能与其他的、隐性的变量相关，或者因变量可能会 "反向 "引起独立变量，或者变量之间存在其他虚假的相关性[15]。 例如，假设溺水死亡人数与某一海滩的冰淇淋销售量之间存在相关性。然而，随着天气越来越热，去游泳的人数和冰淇淋的销售量都会增加，推测溺水死亡人数与去游泳的人数相关。游泳人数的增加可能会导致其他两个变量的增加。

为了对结果进行统计学检验，有必要对实验误差的性质进行假设。一个常见的假设是，误差属于正态分布。中心极限定理支持这样的观点，即在许多情况下这是一个很好的近似。

- 高斯-马尔科夫定理。在一个线性模型中，误差对独立变量的条件期望值为零，不相关且方差相等，任何观测值的线性组合的最佳线性无偏估计器，是其最小二乘估计器。"最佳 "是指参数的最小二乘估计器具有最小方差。当误差都属于同一分布时，方差相等的假设是有效的。

- 如果误差属于正态分布，最小二乘估计器也是线性模型中的最大似然估计器。

然而，假设误差不属于正态分布，则中心极限估计器也是线性模型中的最大似然估计器。在这种情况下，中心极限定理往往还是意味着，只要样本合理地大，参数估计值将近似于正态分布。为此，考虑到误差均值与自变量无关的重要性质，误差项的分布并不是回归分析中的重要问题。具体来说，误差项是否遵循正态分布通常并不重要。

在单位权重的最小二乘计算中，或者在线性回归中，第j个参数的方差，表示$ \operatorname {var}({\hat {\beta }}_{j})$，通常用以下方法估计

$ {\operatorname {var}}({\hat {\beta }}_{j})=\sigma ^{2}([X^{T}X]^{-1})_{jj} \approx {\frac {S}{n-m}}([X^{T}X]^{-1})_{jj}$
其中真实误差方差σ2由基于最小化的二乘目标函数S值的估计值代替，分母n - m为统计自由度，见有效自由度的概括[12]。

如果参数的概率分布是已知的，或作了渐近，就可以找到置信限度。同样，如果残差的概率分布已知或假设，就可以对残差进行统计检验。如果实验误差的概率分布已知或假设，我们可以推导出任何线性组合的因变量的概率分布。当假设误差遵循正态分布时，推导是很容易的，因此意味着参数估计值和残差也将以自变量的值为条件，呈正态分布[12]。

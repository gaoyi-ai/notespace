---
title: Newton method
categories:
- Optimization
tags:
- newton
date: 2021/3/15 10:00:00
updated: 2021/3/15 16:00:00
---

牛顿法的核心思想是对函数进行泰勒展开。

## 牛顿法用于方程求解

对 f(x) 进行一阶泰勒公式展开：
$$
g(x){\approx}g({x_k})+g'({x_k})(x-{x_k})
$$
此时，将非线性方程 g(x) = 0 近似为线性方程：
$$
 g({x_k})+g'({x_k})(x-{x_k})=0
$$
若 f’(x) != 0，则下一次迭代解为：
$$
 {x_{k+1}}={x_k}-\frac{1}{{g'({x_k})}}g({x_k})
$$
牛顿迭代示意图（因此 Newton 迭代法也称为切线法）：

<img src="https://gitee.com/gaoyi-ai/image-bed/raw/master/images/898641-20160307083523241-1600667106.png" style="zoom:150%;" />

## 牛顿法用于函数最优化求解

对 f(x) 进行二阶泰勒公式展开：
$$
           g(x){\approx}g({x_k})+g'({x_k})(x-{x_k})+\frac{1}{2}g''({x_k}){(x-{x_k})^2}
$$
此时，将非线性优化问题 min f(x) 近似为为二次函数的最优化求解问题：
$$
           \min\{g({x_k})+g'({x_k})(x-{x_k})+\frac{1}{2}g''({x_k}){(x-{x_k})^2}\}
$$
对于上式的求解，即二次函数（抛物线函数）求最小值，对上式中的函数求导：
$$
           g'({x_k})+g''({x_k})(x-{x_k})=0 \\
           \Rightarrow{x_{k+1}}={x_k}-\frac{1}{{g''({x_k})}}g'({x_k})
$$
从本质上来讲，最优化求解问题的迭代形式都是： ${x_{k+1}}={x_k}-kg'({x_k})$

其中 k 为系数，$g'({x_k})$为函数的梯度（即函数值上升的方向），那么$-g'({x_k})$为下降的方向，

最优化问题的标准形式是：求目标函数最小值，只要每次迭代沿着下降的方向迭代那么将逐渐达到最优，而牛顿将每次迭代的步长定为：$1/g''({x_k})$。

牛顿法的优点是收敛速度快，缺点是在用牛顿法进行最优化求解的时候需要求解 Hessian 矩阵。因此，如果在目标函数的梯度和 Hessian 矩阵比较好求的时候应使用 Newton 法。

## Newton 法与 Guass-Newton 法之间的联系

对于优化问题 $\min\left\|{f(x)}\right\|)，即\min\{{f^2}(x)\}$，当理论最优值为 0 时候，这个优化问题就变为了函数求解问题：$\min\{{f^2}(x)\}{\Rightarrow}{f^2}(x)=0{\Rightarrow}f(x)=0$ 

结论：当最优化问题的理论最小值为 0 时，Newton 法求解就可变为 Guass-Newton 法求解。 另外：对 f(x) 进行二阶泰勒展开：$f(x)=f(x_k)+J{x_k}+0.5 x_{k}^{'} H{x_k}$

$f(x)$ 乘以 $f(x)^T$ 并忽略二次以上的项：
$$
                       {f^T}(x) f(x)= {{f^T}({x_k}) + {(J{x_k})^T}  + {(0.5{x_k^{'}}H{x_k})^T}} * \{f({x_k})+J{x_k}+0.5{x_{k}^{'}}H{x_k}\} \\
                       {\rm{=}}{f^T}({x_k})f({x_k})+2f({x_k})J{x_k}+x_k^T{J^T}J{x_k}+f({x_k})x_k^TH{x_k} \\
                       ={f^T}({x_k})f({x_k})+2f({x_k})J{x_k}+x_k^T({J^T}J+f({x_k})H){x_k}
$$
因此，当$x_k$在最优解附近时，即满足$f({x_k})=0$，此时可认为：$H={J^T}J$

### 海森矩阵在牛顿法中的应用

一般来说, 牛顿法主要应用在两个方面, 1, 求方程的根; 2, 最优化.

1. 求解方程

并不是所有的方程都有求根公式, 或者求根公式很复杂, 导致求解困难. 利用牛顿法, 可以迭代求解.

原理是利用泰勒公式, 在 $x_0$处展开, 且展开到一阶, 即 $f(x)=f(x_0)+(x–x_0)f′(x0)$

求解方程 $f(x)=0$, 即 $f(x_0)+(x–x_0)f′(x_0)=0$, 求解 $x=x_1=x_0–f(x_0)/f′(x_0)$, 因为这是利用泰勒公式的一阶展开, $f(x)=f(x_0)+(x–x_0)f′(x_0)$处并不是完全相等, 而是近似相等, 这里求得的 $x_1$并不能让 $f(x)=0$, 只能说 $f(x_1)$的值比 $f(x_0)$更接近 $f(x)=0$, 于是乎, 迭代求解的想法就很自然了, 可以进而推出 $x_{n+1}=x_n–f(x_n)/f′(x_n)$, 通过迭代, 这个式子必然在 $f(x^∗)=0$的时候收敛. 整个过程如下图：

![](https://gitee.com/gaoyi-ai/image-bed/raw/master/images/697b070fjw1dvpdvfz24hj.jpg)

2. 最优化

在最优化的问题中, 线性最优化至少可以使用单纯形法 (或称不动点算法) 求解, 但对于非线性优化问题, 牛顿法提供了一种求解的办法. 假设任务是优化一个目标函数 $f$, 求函数 $f$的极大极小问题, 可以转化为求解函数 $f$的导数 $f′=0$的问题, 这样求可以把优化问题看成方程求解问题 ($f′=0$). 剩下的问题就和第一部分提到的牛顿法求解很相似了.

这次为了求解 $f′=0$的根, 首先把 $f(x)$在探索点 $x_n$处泰勒展开, 展开到 2 阶形式进行近似：

$$f(x)=f(x_n)+f′(x_n)(x–x_n)+\frac{f”(x_n)}{2}(x–x_n)^2$$

然后用 $f(x)$的最小点做为新的探索点 $x_{n+1}$，据此，令：

$$f′(x)=f′(x_n)+f”(x_n)(x–x_n)=0$$

求得出迭代公式：

$$x_{n+1}=x_n–f′(x_n)f”(x_n),n=0,1,…$$

一般认为牛顿法可以利用到曲线本身的信息, 比梯度下降法更容易收敛（迭代更少次数）, 如下图是一个最小化一个目标方程的例子, 红色曲线是利用牛顿法迭代求解, 绿色曲线是利用梯度下降法求解.

![](https://gitee.com/gaoyi-ai/image-bed/raw/master/images/20170616223638108)

在上面讨论的是 2 维情况, 高维情况的牛顿迭代公式是：

$$x_{n+1}=x_n–[Hf(x_n)]–1∇f(x_n),n≥0$$

其中 H 是 Hessian 矩阵, 定义见上. 

高维情况依然可以用牛顿迭代求解, 但是问题是 Hessian 矩阵引入的复杂性, 使得牛顿迭代求解的难度大大增加, 但是已经有了解决这个问题的办法就是 Quasi-Newton method, 不再直接计算 hessian 矩阵, 而是每一步的时候使用梯度向量更新 hessian 矩阵的近似.
---
title: Newton's method in optimization
categories:
- Optimization
tags:
- Newton
- wiki
date: 2021/3/10 20:00:17
updated: 2021/3/12 12:00:17
---

# Newton's method in optimization

在微积分中，牛顿法是一种寻找可微分函数F的根的迭代方法，这些根是方程F(x)=0的解；在最优化中，牛顿法应用于可两次微分函数f的导数f′，以寻找导数的根(f′(x)=0的解)，也就是f的静止点，这些解可能是最小点、最大值或鞍点[1]。

## Newton's Method

优化的核心问题是函数的最小化。

## Geometric interpretation

牛顿方法的几何解释是，在每次迭代时，相当于在试值$f(x)$处拟合一个抛物面$x_k$，在该点具有与表面相同的斜率和曲率，然后继续求该抛物面的最大值或最小值(在更高维度上，这也可能是一个鞍点)。 2]注意，如果$f$恰好是一个二次函数，那么精确的极值是一步就能找到的。

## Computing the Newton direction

在高维度上寻找Hessian的反值来计算牛顿方向$h=-(f''(x_{k}))^{-1}f'(x_{k})$可能是一个昂贵的操作。在这种情况下，最好是计算向量h作为线性方程系统$[f''(x_{k})]h=-f'(x_{k})$的解决方案而不是直接将Hessian求反。可以通过各种分解法或使用迭代法近似地（但精度很高）求解。许多这些方法只适用于某些类型的方程，例如Cholesky分解和共轭梯度只有在$f''x_k$是正定矩阵时才能使用。虽然这可能看起来像一个限制，但它通常是一个有用的指标，说明有些东西出了问题；例如，如果正在接近一个最小化问题，而$f''x_k$不是正定矩阵，那么迭代将收敛到一个鞍点，而不是最小值。


---
title: R^2
categories:
- Math
- Statistic
- Regression
tags:
- regression
date: 2021/1/13 10:00:00
updated: 2021/1/21 16:00:00
---

机器学习中关于回归模型有时候需要衡量自变量和因变量之间的相关度，接下来介绍两个衡量相关度的指标：

# 皮尔逊相关系数

它是用来**衡量两个变量之间的相关度**的；取值：[-1,1]

值>0 表示两个变量之间是正相关的，
值为0表示两个变量之间无相关性，
值<0表示两个变量之间是负相关的；

皮尔逊相关系数的计算公式可以表示为：

$$
\begin{equation}
\rho = Cor(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var{(X)}Var{(X)}}}
\end{equation}
$$

# R^2^

在我们学习降维算法PCA的时候，我们提到我们使用方差来衡量数据上的信息量。如果方差越大，代表数据上的信息量越多，而这个信息量不仅包括了数值的大小，还包括了我们希望模型捕捉的那些规律。为了衡量模型对数据上的信息量的捕捉，我们定义了R^2^来帮助我们：
$$
\begin{equation}
R^2 = \frac{SSR}{SST} = \frac{\sum{(\hat{y}_{i}-\bar{y})}^2}{\sum {(y_{i}-\bar{y})}^2}
\end{equation}
$$
其中$y$是我们的真实标签， $\hat{y}$是我们的预测结果， $\bar{y}$是我们的均值， $y_{i}-\bar{y}$如果除以样本量m就是我们的方差。方差的本质是任意一个$y$值和样本均值的差异，差异越大，这些值所带的信息越多。在R^2^中，分子是真实值和预测值之差的差值，也就是我们的模型没有捕获到的信息总量，分母是真实标签所带的信息量，所以其衡量的是1 - 我们的模型没有捕获到的信息量占真实标签中所带的信息量的比例，所以，R^2^越接近1越好。

除了RSS之外，我们还有解释平方和ESS（Explained Sum of Squares，也叫做SSR回归平方和）以及总离差平方和TSS（Total Sum of Squares，也叫做SST总离差平方和）。解释平方和ESS定义了我们的预测值和样本均值之间的差异，而总离差平方和定义了真实值和样本均值之间的差异（就是 R^2^中的分母），两个指标分别写作：
$$
\begin{equation}
TSS = \sum{(y_{i}-\bar{y})^2},ESS = \sum{(\hat{y}_{i}-\bar{y})^2}
\end{equation}
$$
而我们有公式：
$$
TSS = RSS + ESS
$$
看我们的R^2^的公式，如果带入TSS和ESS，那就有：
$$
R^2 = 1 - \frac{RSS}{TSS} = \frac{TSS - RSS}{TSS} = \frac{ESS}{TSS}
$$
而ESS和TSS都带平方，所以必然都是正数，那 R^2^怎么可能是负的呢？

当R^2^为负时，看下面这张图，蓝色的横线是均值线 $\bar{y}$，橙色的线是模型$\hat{y}$ ，蓝色的点是我们的样本点。现在对于 $x_{i}$来说，真实标签减预测值的值$(y_{i}-\hat{y}_{i})$  为正，但预测值$(\hat{y}_{i}-\bar{y})$  却是一个负数，这说明，数据本身的均值，比我们对数据的拟合模型本身更接近数据的真实值，那我们的模型就是废的，完全没有作用，类似于分类模型中的分类准确率为50%，不如瞎猜。

![image-20210121171620934](https://gitee.com/gaoyi-ai/image-bed/raw/master/images/image-20210121171620934.png)

也就是说，当我们的 显示为负的时候，这证明我们的模型对我们的数据的拟合非常糟糕，模型完全不能使用。所以，一个负的R^2^ 是合理的。

但是R平方也会有局限性，R平方会随着样本量的增大而增加，R平方和样本量是有关系的，因此我们要对R平方进行修正：

$$
\begin{equation}
R^2 adjusted = 1 - \frac{(1-R^2)(N-1))}{N-p-1}
\end{equation}
$$

这里N就是样本量，p就是有多少值要进行预测

这样就可以抵消样本量对R平方的影响，实际中经常使用上面的修正值来评估线性模型对数据的拟合程度


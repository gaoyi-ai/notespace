---
title: Stochastic Descent Algorithm with Python and Numpy
categories:
- Optimization
tags:
- SGD
date: 2021/3/12 10:00:00
updated: 2021/3/12 16:00:00
---



[**éšæœºæ¢¯åº¦ä¸‹é™**](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)æ˜¯æœºå™¨å­¦ä¹ åº”ç”¨ä¸­ç»å¸¸ä½¿ç”¨çš„ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºå¯»æ‰¾é¢„æµ‹è¾“å‡ºå’Œå®é™…è¾“å‡ºä¹‹é—´å¯¹åº”çš„æœ€ä½³æ‹Ÿåˆçš„æ¨¡å‹å‚æ•°ã€‚è¿™æ˜¯ä¸€ç§ä¸ç²¾ç¡®ä½†å¼ºå¤§çš„æŠ€æœ¯ã€‚

éšæœºæ¢¯åº¦ä¸‹é™æ³•åœ¨æœºå™¨å­¦ä¹ åº”ç”¨ä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚ç»“åˆ[åå‘ä¼ æ’­](https://brilliant.org/wiki/backpropagation/)ï¼Œå®ƒåœ¨[ç¥ç»ç½‘ç»œ](https://realpython.com/python-keras-text-classification/#a-primer-on-deep-neural-networks)è®­ç»ƒåº”ç”¨ä¸­å ä¸»å¯¼åœ°ä½ã€‚

**In this tutorial, youâ€™ll learn:**

*   How **gradient descent** and **stochastic gradient descent** algorithms work
*   How to apply gradient descent and stochastic gradient descent to **minimize the loss function** in machine learning
*   What the **learning rate** is, why itâ€™s important, and how it impacts results
*   How to **write your own function** for stochastic gradient descent

Basic Gradient Descent Algorithm
--------------------------------------------------------------------------------------

[æ¢¯åº¦ä¸‹é™ç®—æ³•](https://en.wikipedia.org/wiki/Gradient_descent)æ˜¯[æ•°å­¦ä¼˜åŒ–](https://en.wikipedia.org/wiki/Mathematical_optimization)çš„ä¸€ç§è¿‘ä¼¼å’Œè¿­ä»£æ–¹æ³•ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥æ¥è¿‘ä»»ä½•[å¯å¾®åˆ†å‡½æ•°](https://en.wikipedia.org/wiki/Differentiable_function)çš„æœ€å°å€¼ã€‚

è™½ç„¶æ¢¯åº¦ä¸‹é™æ³•æœ‰æ—¶ä¼šå¡åœ¨[å±€éƒ¨æœ€å°å€¼](https://en.wikipedia.org/wiki/Local_optimum)æˆ–[éç‚¹](https://en.wikipedia.org/wiki/Saddle_point)ï¼Œè€Œä¸æ˜¯æ‰¾åˆ°å…¨å±€æœ€å°å€¼ï¼Œä½†å®ƒåœ¨å®è·µä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚[æ•°æ®ç§‘å­¦](https://realpython.com/learning-paths/data-science-python-core-skills/)å’Œ[æœºå™¨å­¦ä¹ ](https://realpython.com/learning-paths/machine-learning-python/)æ–¹æ³•ç»å¸¸åœ¨å†…éƒ¨åº”ç”¨å®ƒæ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚ä¾‹å¦‚ï¼Œç¥ç»ç½‘ç»œç”¨æ¢¯åº¦ä¸‹é™æ³•æ‰¾åˆ°[æƒé‡å’Œåç½®](https://docs.paperspace.com/machine-learning/wiki/weights-and-biases)ã€‚

### Cost Function: The Goal of Optimization

**æˆæœ¬å‡½æ•°**ï¼Œæˆ–[æŸå¤±å‡½æ•°](https://en.wikipedia.org/wiki/Loss_function)ï¼Œæ˜¯æŒ‡é€šè¿‡æ”¹å˜å†³ç­–å˜é‡æ¥æœ€å°åŒ–ï¼ˆæˆ–æœ€å¤§åŒ–ï¼‰çš„å‡½æ•°ã€‚è®¸å¤šæœºå™¨å­¦ä¹ æ–¹æ³•è§£å†³çš„æ˜¯è¡¨é¢ä¸‹çš„ä¼˜åŒ–é—®é¢˜ã€‚å®ƒä»¬å€¾å‘äºé€šè¿‡è°ƒæ•´æ¨¡å‹å‚æ•°ï¼ˆå¦‚[ç¥ç»ç½‘ç»œ](https://en.wikipedia.org/wiki/Artificial_neural_network)çš„æƒé‡å’Œåç½®ã€[éšæœºæ£®æ—](https://en.wikipedia.org/wiki/Random_forest)æˆ–[æ¢¯åº¦æå‡](https://en.wikipedia.org/wiki/Gradient_boosting)çš„å†³ç­–è§„åˆ™ç­‰ï¼‰æ¥æœ€å°åŒ–å®é™…è¾“å‡ºå’Œé¢„æµ‹è¾“å‡ºä¹‹é—´çš„å·®å¼‚ã€‚

åœ¨[å›å½’é—®é¢˜](https://realpython.com/linear-regression-in-python/#regression)ä¸­ï¼Œä½ é€šå¸¸æœ‰è¾“å…¥å˜é‡çš„å‘é‡ğ±=(ğ‘¥â‚ï¼Œ...ï¼Œğ‘¥áµ£)å’Œå®é™…è¾“å‡ºğ‘¦ã€‚ä½ æƒ³æ‰¾åˆ°ä¸€ä¸ªæ¨¡å‹ï¼Œå°†ğ±æ˜ å°„åˆ°é¢„æµ‹å“åº”ğ‘“(ğ±)ï¼Œä½¿ğ‘“(ğ±)å°½å¯èƒ½åœ°æ¥è¿‘äºğ‘¦ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯èƒ½æƒ³é¢„æµ‹ä¸€ä¸ªäººçš„å·¥èµ„ç­‰è¾“å‡ºï¼Œç»™å®šçš„è¾“å…¥æ˜¯è¿™ä¸ªäººåœ¨å…¬å¸çš„å¹´æ•°æˆ–æ•™è‚²æ°´å¹³ã€‚

Your goal is to minimize the difference between the prediction ğ‘“(ğ±) and the actual data ğ‘¦. This difference is called the **residual**.

In this type of problem, you want to minimize the [sum of squared residuals (SSR)](https://en.wikipedia.org/wiki/Residual_sum_of_squares), where SSR = Î£áµ¢(ğ‘¦áµ¢ âˆ’ ğ‘“(ğ±áµ¢))Â² for all observations ğ‘– = 1, â€¦, ğ‘›, where ğ‘› is the total number of observations. Alternatively, you could use the [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) (MSE = SSR / ğ‘›) instead of SSR.

Both SSR and MSE use the square of the difference between the actual and predicted outputs. The lower the difference, the more accurate the prediction. A difference of zero indicates that the prediction is equal to the actual data.

SSR or MSE is minimized by adjusting the model parameters. For example, in [linear regression](https://realpython.com/linear-regression-in-python/), you want to find the function ğ‘“(ğ±) = ğ‘â‚€ + ğ‘â‚ğ‘¥â‚ + â‹¯ + ğ‘áµ£ğ‘¥áµ£, so you need to determine the weights ğ‘â‚€, ğ‘â‚, â€¦, ğ‘áµ£ that minimize SSR or MSE.

åœ¨ä¸€ä¸ª[åˆ†ç±»é—®é¢˜](https://realpython.com/logistic-regression-python/#classification)ä¸­ï¼Œè¾“å‡ºğ‘¦æ˜¯[åˆ†ç±»](https://en.wikipedia.org/wiki/Categorical_variable)ï¼Œé€šå¸¸ä¸æ˜¯0å°±æ˜¯1ã€‚ä¾‹å¦‚ï¼Œä½ å¯èƒ½ä¼šå°è¯•é¢„æµ‹ä¸€å°é‚®ä»¶æ˜¯å¦æ˜¯åƒåœ¾é‚®ä»¶ã€‚åœ¨äºŒè¿›åˆ¶è¾“å‡ºçš„æƒ…å†µä¸‹ï¼Œå°†[äº¤å‰ç†µå‡½æ•°](https://en.wikipedia.org/wiki/Cross_entropy)æœ€å°åŒ–æ˜¯å¾ˆæ–¹ä¾¿çš„ï¼Œå®ƒä¹Ÿå–å†³äºå®é™…è¾“å‡ºğ‘¦áµ¢å’Œç›¸åº”çš„é¢„æµ‹ğ‘(ğ±áµ¢)ã€‚

$$
H=-\sum_{i}\left(y_{i} \log \left(p\left(\mathbf{x}_{i}\right)\right)+\left(1-y_{i}\right) \log \left(1-p\left(\mathbf{x}_{i}\right)\right)\right)
$$
In [logistic regression](https://realpython.com/logistic-regression-python/), which is often used to solve classification problems, the functions ğ‘(ğ±) and ğ‘“(ğ±) are defined as the following:
$$
\begin{array}{c}
p(\mathbf{x})=\frac{1}{1+\exp (-f(\mathbf{x}))} \\
f(\mathbf{x})=b_{0}+b_{1} x_{1}+\cdots+b_{r} x_{r}
\end{array}
$$
Again, you need to find the weights ğ‘â‚€, ğ‘â‚, â€¦, ğ‘áµ£, but this time they should minimize the cross-entropy function.

### Gradient of a Function: Calculus Refresher

åœ¨å¾®ç§¯åˆ†ä¸­ï¼Œå‡½æ•°çš„[å¯¼æ•°](https://www.mathsisfun.com/calculus/derivatives-introduction.html)å‘ä½ å±•ç¤ºäº†å½“ä½ ä¿®æ”¹å®ƒçš„å‚æ•°(æˆ–å‚æ•°)æ—¶ï¼Œä¸€ä¸ªå€¼çš„å˜åŒ–æœ‰å¤šå¤§ã€‚å¯¼æ•°å¯¹ä¼˜åŒ–å¾ˆé‡è¦ï¼Œå› ä¸º[é›¶å¯¼æ•°](http://sofia.nmsu.edu/~breakingaway/ebookofcalculus/MeaningOfDerivativesAndIntegrals/WhatDoesItMeanThatTheDerivativeOfAFunctionEquals0/WhatDoesItMeanThatTheDerivativeOfAFunctionEquals0.html)å¯èƒ½è¡¨ç¤ºæœ€å°ã€æœ€å¤§æˆ–éç‚¹ã€‚

å‡½æ•°ğ¶ çš„å‡ ä¸ªç‹¬ç«‹å˜é‡ğ‘£â‚ï¼Œ...ï¼Œğ‘£áµ£çš„[æ¢¯åº¦](https://en.wikipedia.org/wiki/Gradient)ç”¨âˆ‡ğ¶(ğ‘£â‚ï¼Œ...ï¼Œğ‘£áµ£)è¡¨ç¤ºï¼Œå¹¶å®šä¹‰ä¸ºğ¶ çš„[éƒ¨åˆ†å¯¼æ•°](https://en.wikipedia.org/wiki/Partial_derivative)å¯¹æ¯ä¸ªç‹¬ç«‹å˜é‡çš„å‘é‡å‡½æ•°ï¼šâˆ‡ğ¶ = (âˆ‚ğ¶/âˆ‚ğ‘£â‚ï¼Œ...ï¼Œâˆ‚ğ¶/ğ‘£áµ£)ã€‚ç¬¦å·âˆ‡ç§°ä¸º[nabla](https://en.wikipedia.org/wiki/Nabla_symbol)ã€‚

å‡½æ•°ğ¶åœ¨ç»™å®šç‚¹çš„æ¢¯åº¦çš„éé›¶å€¼å®šä¹‰äº†ğ¶æœ€å¿«å¢åŠ çš„æ–¹å‘å’Œé€Ÿç‡ã€‚å½“ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ—¶ï¼Œä½ æ„Ÿå…´è¶£çš„æ˜¯æˆæœ¬å‡½æ•°ä¸­æœ€å¿«çš„_å‡å°‘çš„æ–¹å‘ã€‚è¿™ä¸ªæ–¹å‘æ˜¯ç”±è´Ÿæ¢¯åº¦ï¼Œ-âˆ‡ğ¶å†³å®šçš„ã€‚

### Intuition Behind Gradient Descent

ä¸ºäº†ç†è§£æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œæƒ³è±¡ä¸€æ»´æ°´ä»ç¢—è¾¹æ»‘ä¸‹ï¼Œæˆ–è€…ä¸€ä¸ªçƒä»å±±ä¸Šæ»šä¸‹ã€‚æ°´æ»´å’Œçƒå€¾å‘äºæ²¿ç€ä¸‹é™æœ€å¿«çš„æ–¹å‘ç§»åŠ¨ï¼Œç›´åˆ°å®ƒä»¬åˆ°è¾¾åº•éƒ¨ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œå®ƒä»¬ä¼šè·å¾—åŠ¨é‡å¹¶åŠ é€Ÿã€‚

æ¢¯åº¦ä¸‹é™èƒŒåçš„æƒ³æ³•ä¸æ­¤ç±»ä¼¼ï¼šä½ ä»ä»»æ„é€‰æ‹©çš„ç‚¹æˆ–å‘é‡ğ¯=ï¼ˆğ‘£â‚ï¼Œ...ï¼Œğ‘£áµ£ï¼‰çš„ä½ç½®å¼€å§‹ï¼Œå¹¶æ²¿ç€æˆæœ¬å‡½æ•°çš„æœ€å¿«ä¸‹é™æ–¹å‘è¿­ä»£ç§»åŠ¨ã€‚å¦‚å‰æ‰€è¿°ï¼Œè¿™å°±æ˜¯è´Ÿæ¢¯åº¦å‘é‡çš„æ–¹å‘ï¼Œ-âˆ‡ğ¶ã€‚

ä¸€æ—¦ä½ æœ‰äº†ä¸€ä¸ªéšæœºçš„èµ·å§‹ç‚¹ğ¯ = (ğ‘£â‚ï¼Œ...ï¼Œğ‘£áµ£)ï¼Œä½ å°±ä¼š**æ›´æ–°**å®ƒï¼Œæˆ–è€…å°†å®ƒç§»åŠ¨åˆ°è´Ÿæ¢¯åº¦æ–¹å‘çš„æ–°ä½ç½®ï¼šğ¯ â†’ ğ¯ - ğœ‚âˆ‡ğ¶ï¼Œå…¶ä¸­ğœ‚(è¯»ä½œ "e-tah")æ˜¯ä¸€ä¸ªå°çš„æ­£å€¼ï¼Œç§°ä¸º**å­¦ä¹ ç‡**ã€‚

å­¦ä¹ ç‡å†³å®šäº†æ›´æ–°æˆ–ç§»åŠ¨æ­¥éª¤çš„å¤§å°ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„å‚æ•°ã€‚å¦‚æœğœ‚å¤ªå°ï¼Œé‚£ä¹ˆç®—æ³•å¯èƒ½æ”¶æ•›å¾—å¾ˆæ…¢ã€‚å¤§çš„ğœ‚å€¼ä¹Ÿä¼šå¯¼è‡´æ”¶æ•›é—®é¢˜æˆ–ä½¿ç®—æ³•å‡ºç°åˆ†æ­§ã€‚

### Implementation of Basic Gradient Descent

Now that you know how the basic gradient descent works, you can implement it in Python. Youâ€™ll use only plain Python and [NumPy](https://numpy.org/), which enables you to write [concise code](https://realpython.com/numpy-array-programming/) when working with arrays (or vectors) and gain a [performance boost](https://realpython.com/numpy-tensorflow-performance/).

This is a basic implementation of the algorithm that starts with an arbitrary point, `start`, iteratively moves it toward the minimum, and [returns](https://realpython.com/python-return-statement/) a point that is hopefully at or near the minimum:

```python
def gradient_descent(gradient, start, learn_rate, n_iter):
    vector = start
    for _ in range(n_iter):
        diff = -learn_rate * gradient(vector)
        vector += diff
    return vector
```

`gradient_descent()` takes four arguments:

1.  **`gradient`** is the [function](https://realpython.com/defining-your-own-python-function/) or any Python [callable object](https://docs.python.org/3/reference/datamodel.html#emulating-callable-objects) that takes a vector and returns the gradient of the function youâ€™re trying to minimize.
2.  **`start`** is the point where the algorithm starts its search, given as a sequence ([tuple, list](https://realpython.com/python-lists-tuples/), [NumPy array](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html), and so on) or scalar (in the case of a one-dimensional problem).
3.  **`learn_rate`** is the learning rate that controls the magnitude of the vector update.
4.  **`n_iter`** is the number of iterations.

This function does exactly whatâ€™s described [above](#intuition-behind-gradient-descent): it takes a starting point (line 2), iteratively updates it according to the learning rate and the value of the gradient (lines 3 to 5), and finally returns the last position found.

Before you apply `gradient_descent()`, you can add another termination criterion:

```python
import numpy as np

def gradient_descent(
    gradient, start, learn_rate, n_iter=50, tolerance=1e-06
):
    vector = start
    for _ in range(n_iter):
        diff = -learn_rate * gradient(vector)
        if np.all(np.abs(diff) <= tolerance):
            break
        vector += diff
    return vector
```

You now have the additional parameter `tolerance` (line 4), which specifies the minimal allowed movement in each iteration. Youâ€™ve also defined the default values for `tolerance` and `n_iter`, so you donâ€™t have to specify them each time you call `gradient_descent()`.

Lines 9 and 10 enable `gradient_descent()` to stop iterating and return the result before `n_iter` is reached if the vector update in the current iteration is less than or equal to `tolerance`. This often happens near the minimum, where gradients are usually very small. Unfortunately, it can also happen near a local minimum or a saddle point.

Line 9 uses the convenient NumPy functions [`numpy.all()`](https://numpy.org/doc/stable/reference/generated/numpy.all.html) and [`numpy.abs()`](https://numpy.org/doc/stable/reference/generated/numpy.absolute.html) to compare the absolute values of `diff` and `tolerance` in a single statement. Thatâ€™s why you `import numpy` on line 1.

Now that you have the first version of `gradient_descent()`, itâ€™s time to test your function. Youâ€™ll start with a small example and find the minimum of the function [ğ¶ = ğ‘£Â²](https://www.wolframalpha.com/input/?i=v**2).

This function has only one independent variable (ğ‘£), and its gradient is the derivative 2ğ‘£. Itâ€™s a differentiable [convex function](https://en.wikipedia.org/wiki/Convex_function), and the analytical way to find its minimum is straightforward. However, in practice, analytical differentiation can be difficult or even impossible and is often approximated with [numerical methods](https://en.wikipedia.org/wiki/Numerical_method).

You need only one statement to test your gradient descent implementation:

```
>>> gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.2
... )
2.210739197207331e-06
```

You use the [lambda function](https://realpython.com/python-lambda/) `lambda v: 2 * v` to provide the gradient of ğ‘£Â². You start from the value `10.0` and set the learning rate to `0.2`. You get a result thatâ€™s very close to zero, which is the correct minimum.

The figure below shows the movement of the solution through the iterations:

<img src="https://gitee.com/gaoyi-ai/image-bed/raw/master/images/gd-1.25c5ef2aed4e.png" style="zoom:67%;" />

You start from the rightmost green dot (ğ‘£ = 10) and move toward the minimum (ğ‘£ = 0). The updates are larger at first because the value of the gradient (and slope) is higher. As you approach the minimum, they become lower.

### Learning Rate Impact

The learning rate is a very important parameter of the algorithm. Different learning rate values can significantly affect the behavior of gradient descent. Consider the previous example, but with a learning rate of 0.8 instead of 0.2:

```
>>> gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.8
... )
-4.77519666596786e-07
```

You get another solution thatâ€™s very close to zero, but the internal behavior of the algorithm is different. This is what happens with the value of ğ‘£ through the iterations:

<img src="https://gitee.com/gaoyi-ai/image-bed/raw/master/images/gd-3.ff9f92989807.png" style="zoom:67%;" />

In this case, you again start with ğ‘£ = 10, but because of the high learning rate, you get a large change in ğ‘£ that passes to the other side of the optimum and becomes âˆ’6. It crosses zero a few more times before settling near it.

Small learning rates can result in very slow convergence. If the number of iterations is limited, then the algorithm may return before the minimum is found. Otherwise, the whole process might take an unacceptably large amount of time. To illustrate this, run `gradient_descent()` again, this time with a much smaller learning rate of 0.005:

```
>>> gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005
... )
6.050060671375367
```

The result is now `6.05`, which is nowhere near the true minimum of zero. This is because the changes in the vector are very small due to the small learning rate:

<img src="https://gitee.com/gaoyi-ai/image-bed/raw/master/images/gd-4.9a5c436570fd.png" style="zoom:67%;" />

The search process starts at ğ‘£ = 10 as before, but it canâ€™t reach zero in fifty iterations. However, with a hundred iterations, the error will be much smaller, and with a thousand iterations, youâ€™ll be very close to zero:

```
>>> gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005,
...     n_iter=100
... )
3.660323412732294
>>> gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005,
...     n_iter=1000
... )
0.0004317124741065828
>>> gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005,
...     n_iter=2000
... )
9.952518849647663e-05
```

Nonconvex functions might have local minima or saddle points where the algorithm can get trapped. In such situations, your choice of learning rate or starting point can make the difference between finding a local minimum and finding the global minimum.

Consider the function [ğ‘£â´ - 5ğ‘£Â² - 3ğ‘£](https://www.wolframalpha.com/input/?i=v**4+-+5+*+v**2+-+3+*+v). It has a global minimum in ğ‘£ â‰ˆ 1.7 and a local minimum in ğ‘£ â‰ˆ âˆ’1.42. The gradient of this function is 4ğ‘£Â³ âˆ’ 10ğ‘£ âˆ’ 3. Letâ€™s see how `gradient_descent()` works here:

```
>>> gradient_descent(
...     gradient=lambda v: 4 * v**3 - 10 * v - 3, start=0,
...     learn_rate=0.2
... )
-1.4207567437458342
```

You started at zero this time, and the algorithm ended near the local minimum. Hereâ€™s what happened under the hood:

<img src="https://gitee.com/gaoyi-ai/image-bed/raw/master/images/gd-7.67e03e9337db.png" style="zoom:67%;" />

During the first two iterations, your vector was moving toward the global minimum, but then it crossed to the opposite side and stayed trapped in the local minimum. You can prevent this with a smaller learning rate:

```
>>> gradient_descent(
...     gradient=lambda v: 4 * v**3 - 10 * v - 3, start=0,
...     learn_rate=0.1
... )
1.285401330315467
```

When you decrease the learning rate from `0.2` to `0.1`, you get a solution very close to the global minimum. Remember that gradient descent is an approximate method. This time, you avoid the jump to the other side:

<img src="https://gitee.com/gaoyi-ai/image-bed/raw/master/images/gd-8.f055cad0b634.png" style="zoom:67%;" />

A lower learning rate prevents the vector from making large jumps, and in this case, the vector remains closer to the global optimum.

Adjusting the learning rate is tricky. You canâ€™t know the best value in advance. There are many techniques and heuristics that try to help with this. In addition, machine learning practitioners often tune the learning rate during model selection and evaluation.

Besides the learning rate, the starting point can affect the solution significantly, especially with nonconvex functions.

Application of the Gradient Descent Algorithm
----------------------------------------------------------------------------------------------------------------

In this section, youâ€™ll see two short examples of using gradient descent. Youâ€™ll also learn that it can be used in real-life machine learning problems like linear regression. In the second case, youâ€™ll need to modify the code of `gradient_descent()` because you need the data from the observations to calculate the gradient.

### Short Examples

First, youâ€™ll apply `gradient_descent()` to another one-dimensional problem. Take the function [ğ‘£ âˆ’ log(ğ‘£)](https://www.wolframalpha.com/input/?i=v+-+log%28v%29). The gradient of this function is 1 âˆ’ 1/ğ‘£. With this information, you can find its minimum:

```
>>> gradient_descent(
...     gradient=lambda v: 1 - 1 / v, start=2.5, learn_rate=0.5
... )
1.0000011077232125
```

With the provided set of arguments, `gradient_descent()` correctly calculates that this function has the minimum in ğ‘£ = 1. You can try it with other values for the learning rate and starting point.

You can also use `gradient_descent()` with functions of more than one variable. The application is the same, but you need to provide the gradient and starting points as vectors or arrays. For example, you can find the minimum of the function [ğ‘£â‚Â² + ğ‘£â‚‚â´](https://www.wolframalpha.com/input/?i=v_1**2+%2B+v_2**4) that has the gradient vector (2ğ‘£â‚, 4ğ‘£â‚‚Â³):

```
>>> gradient_descent(
...     gradient=lambda v: np.array([2 * v[0], 4 * v[1]**3]),
...     start=np.array([1.0, 1.0]), learn_rate=0.2, tolerance=1e-08
... )
array([8.08281277e-12, 9.75207120e-02])
```

In this case, your gradient function returns an array, and the start value is an array, so you get an array as the result. The resulting values are almost equal to zero, so you can say that `gradient_descent()` correctly found that the minimum of this function is at ğ‘£â‚ = ğ‘£â‚‚ = 0.

### Ordinary Least Squares

As youâ€™ve already learned, linear regression and the [ordinary least squares method](https://en.wikipedia.org/wiki/Ordinary_least_squares) start with the observed values of the inputs ğ± = (ğ‘¥â‚, â€¦, ğ‘¥áµ£) and outputs ğ‘¦. They define a linear function ğ‘“(ğ±) = ğ‘â‚€ + ğ‘â‚ğ‘¥â‚ + â‹¯ + ğ‘áµ£ğ‘¥áµ£, which is as close as possible to ğ‘¦.

This is an optimization problem. It finds the values of weights ğ‘â‚€, ğ‘â‚, â€¦, ğ‘áµ£ that minimize the sum of squared residuals SSR = Î£áµ¢(ğ‘¦áµ¢ âˆ’ ğ‘“(ğ±áµ¢))Â² or the mean squared error MSE = SSR / ğ‘›. Here, ğ‘› is the total number of observations and ğ‘– = 1, â€¦, ğ‘›.

You can also use the cost function ğ¶ = SSR / (2ğ‘›), which is mathematically more convenient than SSR or MSE.

The most basic form of linear regression is [simple linear regression](https://realpython.com/linear-regression-in-python/#simple-linear-regression). It has only one set of inputs ğ‘¥ and two weights: ğ‘â‚€ and ğ‘â‚. The equation of the regression line is ğ‘“(ğ‘¥) = ğ‘â‚€ + ğ‘â‚ğ‘¥. Although the optimal values of ğ‘â‚€ and ğ‘â‚ can be [calculated analytically](https://en.wikipedia.org/wiki/Ordinary_least_squares#Simple_linear_regression_model), youâ€™ll use gradient descent to determine them.

First, you need calculus to find the gradient of the cost function ğ¶ = Î£áµ¢(ğ‘¦áµ¢ âˆ’ ğ‘â‚€ âˆ’ ğ‘â‚ğ‘¥áµ¢)Â² / (2ğ‘›). Since you have two decision variables, ğ‘â‚€ and ğ‘â‚, the gradient âˆ‡ğ¶ is a vector with two components:

1.  âˆ‚ğ¶/âˆ‚ğ‘â‚€ = (1/ğ‘›) Î£áµ¢(ğ‘â‚€ + ğ‘â‚ğ‘¥áµ¢ âˆ’ ğ‘¦áµ¢) = mean(ğ‘â‚€ + ğ‘â‚ğ‘¥áµ¢ âˆ’ ğ‘¦áµ¢)
2.  âˆ‚ğ¶/âˆ‚ğ‘â‚ = (1/ğ‘›) Î£áµ¢(ğ‘â‚€ + ğ‘â‚ğ‘¥áµ¢ âˆ’ ğ‘¦áµ¢) ğ‘¥áµ¢ = mean((ğ‘â‚€ + ğ‘â‚ğ‘¥áµ¢ âˆ’ ğ‘¦áµ¢) ğ‘¥áµ¢)

You need the values of ğ‘¥ and ğ‘¦ to calculate the gradient of this cost function. Your gradient function will have as inputs not only ğ‘â‚€ and ğ‘â‚ but also ğ‘¥ and ğ‘¦. This is how it might look:

```python
def ssr_gradient(x, y, b):
    res = b[0] + b[1] * x - y
    return res.mean(), (res * x).mean()  # .mean() is a method of np.ndarray
```

`ssr_gradient()` takes the arrays `x` and `y`, which contain the observation inputs and outputs, and the array `b` that holds the current values of the decision variables ğ‘â‚€ and ğ‘â‚. This function first calculates the array of the residuals for each observation (`res`) and then returns the pair of values of âˆ‚ğ¶/âˆ‚ğ‘â‚€ and âˆ‚ğ¶/âˆ‚ğ‘â‚.

In this example, you can use the convenient NumPy method [`ndarray.mean()`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.mean.html) since you pass NumPy arrays as the arguments.

`gradient_descent()` needs two small adjustments:

1.  Add `x` and `y` as the parameters of `gradient_descent()` on line 4.
2.  Provide `x` and `y` to the gradient function and make sure you convert your gradient tuple to a NumPy array on line 8.

Hereâ€™s how `gradient_descent()` looks after these changes:

```python
import numpy as np

def gradient_descent(
    gradient, x, y, start, learn_rate=0.1, n_iter=50, tolerance=1e-06
):
    vector = start
    for _ in range(n_iter):
        diff = -learn_rate * np.array(gradient(x, y, vector))
        if np.all(np.abs(diff) <= tolerance):
            break
        vector += diff
    return vector
```

`gradient_descent()` now accepts the observation inputs `x` and outputs `y` and can use them to calculate the gradient. Converting the output of `gradient(x, y, vector)` to a NumPy array enables elementwise multiplication of the gradient elements by the learning rate, which isnâ€™t necessary in the case of a single-variable function.

Now apply your new version of `gradient_descent()` to find the regression line for some arbitrary values of `x` and `y`:

```
>>> x = np.array([5, 15, 25, 35, 45, 55])
>>> y = np.array([5, 20, 14, 32, 22, 38])

>>> gradient_descent(
...     ssr_gradient, x, y, start=[0.5, 0.5], learn_rate=0.0008,
...     n_iter=100_000
... )
array([5.62822349, 0.54012867])
```

The result is an array with two values that correspond to the decision variables: ğ‘â‚€ = 5.63 and ğ‘â‚ = 0.54. The best regression line is ğ‘“(ğ‘¥) = 5.63 + 0.54ğ‘¥. As in the previous examples, this result heavily depends on the learning rate. You might not get such a good result with too low or too high of a learning rate.

This example isnâ€™t entirely randomâ€“itâ€™s taken from the tutorial [Linear Regression in Python](https://realpython.com/linear-regression-in-python/#simple-linear-regression-with-scikit-learn). The good news is that youâ€™ve obtained almost the same result as the [linear regressor from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). The data and regression results are visualized in the section [Simple Linear Regression](https://realpython.com/linear-regression-in-python/#simple-linear-regression).

### Improvement of the Code

You can make `gradient_descent()` more robust, comprehensive, and better-looking without modifying its core functionality:

```python
import numpy as np

def gradient_descent(
    gradient, x, y, start, learn_rate=0.1, n_iter=50, tolerance=1e-06,
    dtype="float64"
):
    # Checking if the gradient is callable
    if not callable(gradient):
        raise TypeError("'gradient' must be callable")

    # Setting up the data type for NumPy arrays
    dtype_ = np.dtype(dtype)

    # Converting x and y to NumPy arrays
    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)
    if x.shape[0] != y.shape[0]:
        raise ValueError("'x' and 'y' lengths do not match")

    # Initializing the values of the variables
    vector = np.array(start, dtype=dtype_)

    # Setting up and checking the learning rate
    learn_rate = np.array(learn_rate, dtype=dtype_)
    if np.any(learn_rate <= 0):
        raise ValueError("'learn_rate' must be greater than zero")

    # Setting up and checking the maximal number of iterations
    n_iter = int(n_iter)
    if n_iter <= 0:
        raise ValueError("'n_iter' must be greater than zero")

    # Setting up and checking the tolerance
    tolerance = np.array(tolerance, dtype=dtype_)
    if np.any(tolerance <= 0):
        raise ValueError("'tolerance' must be greater than zero")

    # Performing the gradient descent loop
    for _ in range(n_iter):
        # Recalculating the difference
        diff = -learn_rate * np.array(gradient(x, y, vector), dtype_)

        # Checking if the absolute difference is small enough
        if np.all(np.abs(diff) <= tolerance):
            break

        # Updating the values of the variables
        vector += diff

    return vector if vector.shape else vector.item()
```

`gradient_descent()` now accepts an additional `dtype` parameter that defines the data type of NumPy arrays inside the function. For more information about NumPy types, see the [official documentation on data types](https://numpy.org/doc/stable/user/basics.types.html).

In most applications, you wonâ€™t notice a difference between 32-bit and 64-bit floating-point numbers, but when you work with big datasets, this might significantly affect memory use and maybe even [processing speed](https://stackoverflow.com/questions/15340781/python-numpy-data-types-performance). For example, although NumPy uses 64-bit floats by default, [TensorFlow often uses 32-bit decimal numbers](https://www.tensorflow.org/guide/tensor).

In addition to considering data types, the code above introduces a few modifications related to type checking and ensuring the use of NumPy capabilities:

*   **Lines 8 and 9** check if `gradient` is a Python callable object and whether it can be used as a function. If not, then the function will raise a [`TypeError`](https://docs.python.org/3/library/exceptions.html#TypeError).
    
*   **Line 12** sets an instance of [`numpy.dtype`](https://numpy.org/doc/stable/reference/generated/numpy.dtype.html), which will be used as the data type for all arrays throughout the function.
    
*   **Line 15** takes the arguments `x` and `y` and produces NumPy arrays with the desired data type. The arguments `x` and `y` can be lists, tuples, arrays, or other sequences.
    
*   **Lines 16 and 17** compare the sizes of `x` and `y`. This is useful because you want to be sure that both arrays have the same number of observations. If they donâ€™t, then the function will raise a [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError).
    
*   **Line 20** converts the argument `start` to a NumPy array. This is an interesting trick: if `start` is a Python scalar, then itâ€™ll be transformed into a corresponding NumPy object (an array with one item and zero dimensions). If you pass a sequence, then itâ€™ll become a regular NumPy array with the same number of elements.
    
*   **Line 23** does the same thing with the learning rate. This can be very useful because it enables you to specify different learning rates for each decision variable by passing a list, tuple, or NumPy array to `gradient_descent()`.
    
*   **Lines 24 and 25** check if the learning rate value (or values for all variables) is greater than zero.
    
*   **Lines 28 to 35** similarly set `n_iter` and `tolerance` and check that they are greater than zero.
    
*   **Lines 38 to 47** are almost the same as before. The only difference is the type of the gradient array on line 40.
    
*   **Line 49** conveniently returns the resulting array if you have several decision variables or a Python scalar if you have a single variable.
    

Your `gradient_descent()` is now finished. Feel free to add some additional capabilities or polishing. The next step of this tutorial is to use what youâ€™ve learned so far to implement the stochastic version of gradient descent.

Stochastic Gradient Descent Algorithms
--------------------------------------------------------------------------------------------------

**Stochastic gradient descent algorithms** are a modification of gradient descent. In stochastic gradient descent, you calculate the gradient using just a random small part of the observations instead of all of them. In some cases, this approach can reduce computation time.

**Online stochastic gradient descent** is a variant of stochastic gradient descent in which you estimate the gradient of the cost function for each observation and update the decision variables accordingly. This can help you find the global minimum, especially if the objective function is convex.

**Batch stochastic gradient descent** is somewhere between ordinary gradient descent and the online method. The gradients are calculated and the decision variables are updated iteratively with subsets of all observations, called **minibatches**. This variant is very popular for training neural networks.

You can imagine the online algorithm as a special kind of batch algorithm in which each minibatch has only one observation. Classical gradient descent is another special case in which thereâ€™s only one batch containing all observations.

### Minibatches in Stochastic Gradient Descent

As in the case of the ordinary gradient descent, stochastic gradient descent starts with an initial vector of decision variables and updates it through several iterations. The difference between the two is in what happens inside the iterations:

*   Stochastic gradient descent randomly divides the set of observations into minibatches.
*   For each minibatch, the gradient is computed and the vector is moved.
*   Once all minibatches are used, you say that the iteration, or **epoch**, is finished and start the next one.

This algorithm randomly selects observations for minibatches, so you need to simulate this random (or pseudorandom) behavior. You can do that with [random number generation](https://realpython.com/python-random/). Python has the built-in [`random`](https://docs.python.org/3/library/random.html) module, and NumPy has its own [random generator](https://numpy.org/doc/stable/reference/random/generator.html). The latter is more convenient when you work with arrays.

Youâ€™ll create a new function called `sgd()` that is very similar to `gradient_descent()` but uses randomly selected minibatches to move along the search space:

```python
import numpy as np

def sgd(
    gradient, x, y, start, learn_rate=0.1, batch_size=1, n_iter=50,
    tolerance=1e-06, dtype="float64", random_state=None
):
    # Checking if the gradient is callable
    if not callable(gradient):
        raise TypeError("'gradient' must be callable")

    # Setting up the data type for NumPy arrays
    dtype_ = np.dtype(dtype)

    # Converting x and y to NumPy arrays
    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)
    n_obs = x.shape[0]
    if n_obs != y.shape[0]:
        raise ValueError("'x' and 'y' lengths do not match")
    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]

    # Initializing the random number generator
    seed = None if random_state is None else int(random_state)
    rng = np.random.default_rng(seed=seed)

    # Initializing the values of the variables
    vector = np.array(start, dtype=dtype_)

    # Setting up and checking the learning rate
    learn_rate = np.array(learn_rate, dtype=dtype_)
    if np.any(learn_rate <= 0):
        raise ValueError("'learn_rate' must be greater than zero")

    # Setting up and checking the size of minibatches
    batch_size = int(batch_size)
    if not 0 < batch_size <= n_obs:
        raise ValueError(
            "'batch_size' must be greater than zero and less than "
            "or equal to the number of observations"
        )

    # Setting up and checking the maximal number of iterations
    n_iter = int(n_iter)
    if n_iter <= 0:
        raise ValueError("'n_iter' must be greater than zero")

    # Setting up and checking the tolerance
    tolerance = np.array(tolerance, dtype=dtype_)
    if np.any(tolerance <= 0):
        raise ValueError("'tolerance' must be greater than zero")

    # Performing the gradient descent loop
    for _ in range(n_iter):
        # Shuffle x and y
        rng.shuffle(xy)

        # Performing minibatch moves
        for start in range(0, n_obs, batch_size):
            stop = start + batch_size
            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]

            # Recalculating the difference
            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)
            diff = -learn_rate * grad

            # Checking if the absolute difference is small enough
            if np.all(np.abs(diff) <= tolerance):
                break

            # Updating the values of the variables
            vector += diff

    return vector if vector.shape else vector.item()
```

You have a new parameter here. With `batch_size`, you specify the number of observations in each minibatch. This is an essential parameter for stochastic gradient descent that can significantly affect performance. Lines 34 to 39 ensure that `batch_size` is a positive integer no larger than the total number of observations.

Another new parameter is `random_state`. It defines the seed of the random number generator on line 22. The seed is used on line 23 as an argument to [`default_rng()`](https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.default_rng), which creates an instance of [`Generator`](https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator).

If you pass the argument [`None`](https://realpython.com/null-in-python/) for `random_state`, then the random number generator will return different numbers each time itâ€™s instantiated. If you want each instance of the generator to behave exactly the same way, then you need to specify `seed`. The easiest way is to provide an arbitrary integer.

Line 16 deduces the number of observations with `x.shape[0]`. If `x` is a one-dimensional array, then this is its size. If `x` has two dimensions, then `.shape[0]` is the number of rows.

On line 19, you use [`.reshape()`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html#numpy.ndarray.reshape) to make sure that both `x` and `y` become two-dimensional arrays with `n_obs` rows and that `y` has exactly one column. [`numpy.c_[]`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html#numpy.ndarray.reshape) conveniently concatenates the columns of `x` and `y` into a single array, `xy`. This is one way to make data suitable for random selection.

Finally, on lines 52 to 70, you implement the [`for` loop](https://realpython.com/python-for-loop/) for the stochastic gradient descent. It differs from `gradient_descent()`. On line 54, you use the random number generator and its method [`.shuffle()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.shuffle.html#numpy.random.Generator.shuffle) to shuffle the observations. This is one of the ways to choose minibatches randomly.

The inner `for` loop is repeated for each minibatch. The main difference from the ordinary gradient descent is that, on line 62, the gradient is calculated for the observations from a minibatch (`x_batch` and `y_batch`) instead of for all observations (`x` and `y`).

On line 59, `x_batch` becomes a part of `xy` that contains the rows of the current minibatch (from `start` to `stop`) and the columns that correspond to `x`. `y_batch` holds the same rows from `xy` but only the last column (the outputs). For more information about how indices work in NumPy, see the [official documentation on indexing](https://numpy.org/doc/stable/reference/arrays.indexing.html).

Now you can test your implementation of stochastic gradient descent:

```
>>> sgd(
...     ssr_gradient, x, y, start=[0.5, 0.5], learn_rate=0.0008,
...     batch_size=3, n_iter=100_000, random_state=0
... )
array([5.63093736, 0.53982921])
```

The result is almost the same as you got with `gradient_descent()`. If you omit `random_state` or use `None`, then youâ€™ll get somewhat different results each time you run `sgd()` because the random number generator will shuffle `xy` differently.

### Momentum in Stochastic Gradient Descent

As youâ€™ve already seen, the learning rate can have a significant impact on the result of gradient descent. You can use several different strategies for adapting the learning rate during the algorithm execution. You can also apply **momentum** to your algorithm.

You can use momentum to correct the effect of the learning rate. The idea is to remember the previous update of the vector and apply it when calculating the next one. You donâ€™t move the vector exactly in the direction of the negative gradient, but you also tend to keep the direction and magnitude from the previous move.

The parameter called the **decay rate** or **decay factor** defines how strong the contribution of the previous update is. To include the momentum and the decay rate, you can modify `sgd()` by adding the parameter `decay_rate` and use it to calculate the direction and magnitude of the vector update (`diff`):

```python
import numpy as np

def sgd(
    gradient, x, y, start, learn_rate=0.1, decay_rate=0.0, batch_size=1,
    n_iter=50, tolerance=1e-06, dtype="float64", random_state=None
):
    # Checking if the gradient is callable
    if not callable(gradient):
        raise TypeError("'gradient' must be callable")

    # Setting up the data type for NumPy arrays
    dtype_ = np.dtype(dtype)

    # Converting x and y to NumPy arrays
    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)
    n_obs = x.shape[0]
    if n_obs != y.shape[0]:
        raise ValueError("'x' and 'y' lengths do not match")
    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]

    # Initializing the random number generator
    seed = None if random_state is None else int(random_state)
    rng = np.random.default_rng(seed=seed)

    # Initializing the values of the variables
    vector = np.array(start, dtype=dtype_)

    # Setting up and checking the learning rate
    learn_rate = np.array(learn_rate, dtype=dtype_)
    if np.any(learn_rate <= 0):
        raise ValueError("'learn_rate' must be greater than zero")

    # Setting up and checking the decay rate
    decay_rate = np.array(decay_rate, dtype=dtype_)
    if np.any(decay_rate < 0) or np.any(decay_rate > 1):
        raise ValueError("'decay_rate' must be between zero and one")

    # Setting up and checking the size of minibatches
    batch_size = int(batch_size)
    if not 0 < batch_size <= n_obs:
        raise ValueError(
            "'batch_size' must be greater than zero and less than "
            "or equal to the number of observations"
        )

    # Setting up and checking the maximal number of iterations
    n_iter = int(n_iter)
    if n_iter <= 0:
        raise ValueError("'n_iter' must be greater than zero")

    # Setting up and checking the tolerance
    tolerance = np.array(tolerance, dtype=dtype_)
    if np.any(tolerance <= 0):
        raise ValueError("'tolerance' must be greater than zero")

    # Setting the difference to zero for the first iteration
    diff = 0

    # Performing the gradient descent loop
    for _ in range(n_iter):
        # Shuffle x and y
        rng.shuffle(xy)

        # Performing minibatch moves
        for start in range(0, n_obs, batch_size):
            stop = start + batch_size
            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]

            # Recalculating the difference
            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)
            diff = decay_rate * diff - learn_rate * grad

            # Checking if the absolute difference is small enough
            if np.all(np.abs(diff) <= tolerance):
                break

            # Updating the values of the variables
            vector += diff

    return vector if vector.shape else vector.item()
```

In this implementation, you add the `decay_rate` parameter on line 4, convert it to a NumPy array of the desired type on line 34, and check if itâ€™s between zero and one on lines 35 and 36. On line 57, you initialize `diff` before the iterations start to ensure that itâ€™s available in the first iteration.

The most important change happens on line 71. You recalculate `diff` with the learning rate and gradient but also add the product of the decay rate and the old value of `diff`. Now `diff` has two components:

1.  **`decay_rate * diff`** is the momentum, or impact of the previous move.
2.  **`-learn_rate * grad`** is the impact of the current gradient.

The decay and learning rates serve as the weights that define the contributions of the two.

### Random Start Values

As opposed to ordinary gradient descent, the starting point is often not so important for stochastic gradient descent. It may also be an unnecessary difficulty for a user, especially when you have many decision variables. To get an idea, just imagine if you needed to manually initialize the values for a neural network with thousands of biases and weights!

In practice, you can start with some small arbitrary values. Youâ€™ll use the random number generator to get them:

```python
import numpy as np

def sgd(
    gradient, x, y, n_vars=None, start=None, learn_rate=0.1,
    decay_rate=0.0, batch_size=1, n_iter=50, tolerance=1e-06,
    dtype="float64", random_state=None
):
    # Checking if the gradient is callable
    if not callable(gradient):
        raise TypeError("'gradient' must be callable")

    # Setting up the data type for NumPy arrays
    dtype_ = np.dtype(dtype)

    # Converting x and y to NumPy arrays
    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)
    n_obs = x.shape[0]
    if n_obs != y.shape[0]:
        raise ValueError("'x' and 'y' lengths do not match")
    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]

    # Initializing the random number generator
    seed = None if random_state is None else int(random_state)
    rng = np.random.default_rng(seed=seed)

    # Initializing the values of the variables
    vector = (
        rng.normal(size=int(n_vars)).astype(dtype_)
        if start is None else
        np.array(start, dtype=dtype_)
    )

    # Setting up and checking the learning rate
    learn_rate = np.array(learn_rate, dtype=dtype_)
    if np.any(learn_rate <= 0):
        raise ValueError("'learn_rate' must be greater than zero")

    # Setting up and checking the decay rate
    decay_rate = np.array(decay_rate, dtype=dtype_)
    if np.any(decay_rate < 0) or np.any(decay_rate > 1):
        raise ValueError("'decay_rate' must be between zero and one")

    # Setting up and checking the size of minibatches
    batch_size = int(batch_size)
    if not 0 < batch_size <= n_obs:
        raise ValueError(
            "'batch_size' must be greater than zero and less than "
            "or equal to the number of observations"
        )

    # Setting up and checking the maximal number of iterations
    n_iter = int(n_iter)
    if n_iter <= 0:
        raise ValueError("'n_iter' must be greater than zero")

    # Setting up and checking the tolerance
    tolerance = np.array(tolerance, dtype=dtype_)
    if np.any(tolerance <= 0):
        raise ValueError("'tolerance' must be greater than zero")

    # Setting the difference to zero for the first iteration
    diff = 0

    # Performing the gradient descent loop
    for _ in range(n_iter):
        # Shuffle x and y
        rng.shuffle(xy)

        # Performing minibatch moves
        for start in range(0, n_obs, batch_size):
            stop = start + batch_size
            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]

            # Recalculating the difference
            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)
            diff = decay_rate * diff - learn_rate * grad

            # Checking if the absolute difference is small enough
            if np.all(np.abs(diff) <= tolerance):
                break

            # Updating the values of the variables
            vector += diff

    return vector if vector.shape else vector.item()
```

You now have the new parameter `n_vars` that defines the number of decision variables in your problem. The parameter `start` is optional and has the default value `None`. Lines 27 to 31 initialize the starting values of the decision variables:

*   If you provide a `start` value other than `None`, then itâ€™s used for the starting values.
*   If `start` is `None`, then your random number generator creates the starting values using the [standard normal distribution](https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution) and the NumPy method [`.normal()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html).

Now give `sgd()` a try:

```
>>> sgd(
...     ssr_gradient, x, y, n_vars=2, learn_rate=0.0001,
...     decay_rate=0.8, batch_size=3, n_iter=100_000, random_state=0
... )
array([5.63014443, 0.53901017])
```

You get similar results again.

Youâ€™ve learned how to write the functions that implement gradient descent and stochastic gradient descent. The code above can be made more robust and polished. You can also find different implementations of these methods in well-known machine learning libraries.

Gradient Descent in Keras and TensorFlow
------------------------------------------------------------------------------------------------------

Stochastic gradient descent is widely used to train neural networks. The libraries for neural networks often have different variants of optimization algorithms based on stochastic gradient descent, such as:

*   Adam
*   Adagrad
*   Adadelta
*   RMSProp

These optimization libraries are usually called internally when neural network software is trained. However, you can use them independently as well:

```
>>> import tensorflow as tf

>>> # Create needed objects
>>> sgd = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)
>>> var = tf.Variable(2.5)
>>> cost = lambda: 2 + var ** 2

>>> # Perform optimization
>>> for _ in range(100):
...     sgd.minimize(cost, var_list=[var])

>>> # Extract results
>>> var.numpy()
-0.007128528
>>> cost().numpy()
2.0000508
```

In this example, you first import `tensorflow` and then create the object needed for optimization:

*   **`sgd`** is an instance of the stochastic gradient descent optimizer with a learning rate of `0.1` and a momentum of `0.9`.
*   **`var`** is an instance of the decision variable with an initial value of `2.5`.
*   **`cost`** is the cost function, which is a square function in this case.

The main part of the code is a `for` loop that iteratively calls `.minimize()` and modifies `var` and `cost`. Once the loop is exhausted, you can get the values of the decision variable and the cost function with `.numpy()`.

You can find more information on these algorithms in the [Keras](https://keras.io/api/optimizers/) and [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/) documentation. The article [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/) offers a comprehensive list with explanations of gradient descent variants.

Conclusion
------------------------------------------

You now know what **gradient descent** and **stochastic gradient descent** algorithms are and how they work. Theyâ€™re widely used in the applications of artificial neural networks and are implemented in popular libraries like Keras and TensorFlow.

**In this tutorial, youâ€™ve learned:**

*   How to **write your own functions** for gradient descent and stochastic gradient descent
*   How to **apply your functions** to solve optimization problems
*   What the **key features and concepts** of gradient descent are, like learning rate or momentum, as well as its limitations

Youâ€™ve used gradient descent and stochastic gradient descent to find the minima of several functions and to fit the regression line in a linear regression problem. Youâ€™ve also seen how to apply the class `SGD` from TensorFlow thatâ€™s used to train neural networks.

If you have questions or comments, then please put them in the comment section below.

> [realpython.com/gradient-descent-algorithm-python](https://realpython.com/gradient-descent-algorithm-python/)
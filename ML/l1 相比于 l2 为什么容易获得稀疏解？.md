---
title: l1 vs l2 norm
categories:
- ML
- Norm
tags:
- norm
date: 2021/3/30 10:00:00
updated: 2021/3/30 16:00:00
---



假设费用函数 L 与某个参数 x 的关系如图所示：  

![](https://gitee.com/gaoyi-ai/image-bed/raw/master/images/40de2e79cf8af8a9f75ba2d48ae05f16_r.jpg) 
则最优的 x 在绿点处，x 非零。

现在施加 L2 regularization，新的费用函数（$L+Cx^2$）如图中蓝线所示：  

![](https://gitee.com/gaoyi-ai/image-bed/raw/master/images/6221f45c527e0fc4c0d38a4ef30ee241_r.jpg)

最优的 x 在黄点处，x 的绝对值减小了，但依然非零。

而如果施加 L1 regularization，则新的费用函数（$L+C|x|$）如图中粉线所示：  

![](https://gitee.com/gaoyi-ai/image-bed/raw/master/images/d534de56d13bf7d226a4d654c1ab02f0_r.jpg)

最优的 x 就变成了 0。这里利用的就是绝对值函数的尖峰。

两种 regularization 能不能把最优的 x 变成 0，取决于原先的费用函数在 0 点处的导数。 
如果本来导数不为 0，那么施加 L2 regularization 后导数依然不为 0，最优的 x 也不会变成 0。 

而施加 L1 regularization 时，只要 regularization 项的系数 C 大于原先费用函数在 0 点处的导数的绝对值，x = 0 就会变成一个极小值点。即, 要形成极小值点，以上图为例，x<0 时 L+C|x| 的导数要小于0(函数减)，同理x>0 时导数>0 (函数增) x从左边趋近于0 时，C|x|的导数是-C，假设此时 L 的导数为 La ，必须有 La -C <0，即C>La，同理x从右边趋近于0时，必须有 Lb + C > 0 ，即C>-Lb，所以说C要大于L在0点附近的绝对值

上面只分析了一个参数 x。事实上 L1 regularization 会使得许多参数的最优值变成 0，这样模型就稀疏了。

---

![](https://gitee.com/gaoyi-ai/image-bed/raw/master/images/v2-a026e24156e13a1d14c43df26b9bd2a4_r.jpg)![](https://pic1.zhimg.com/v2-f6edae58134c5a26687c3883af48d5d5_r.jpg?source=1940ef5c)![](https://pic4.zhimg.com/v2-3aaa69f70754c469bca5c8e4c3e161db_r.jpg?source=1940ef5c)

> [www.zhihu.com](https://www.zhihu.com/question/37096933/answer/70507353)
---
title: The sample imbalance problem
categories:
- ML
- Imbalance 
tags:
- imbalance
date: 2021/4/13 10:00:00
updated: 2021/4/13 16:00:00
---



> [zhuanlan.zhihu.com](https://zhuanlan.zhihu.com/p/321323696)

摘要：本篇主要从理论到实践解决文本分类中的样本不均衡问题。首先讲了下什么是样本不均衡现象以及可能带来的问题；然后重点从数据层面和模型层面讲解样本不均衡问题的解决策略。数据层面主要通过欠采样和过采样的方式来人为调节正负样本比例，模型层面主要是通过加权 Loss，包括基于类别 Loss、Focal Loss 和 GHM Loss 三种加权 Loss 函数；最后讲了下其他解决样本不均衡的策略，可以通过调节阈值修改正负样本比例和利用半监督或自监督学习解决样本不均衡问题。需要说明下上面解决样本不均衡问题的策略不仅仅适用于文本分类任务，还可以扩展到其他的机器学习任务中。对于希望解决样本不均衡问题的小伙伴可能有所帮助。
**01 机器学习中的样本不均衡问题**  

1.1 什么是样本不均衡现象

机器学习领域中样本不均衡现象随处可见。咱们举一些例子说明，图片分类任务中假如我们要做猫狗图片的识别任务，因为猫狗在日常生活中随处可见，所以对应的样本猫和狗的图片很好找，样本比较均衡，咱们能很容易的得到 1W 张猫的图片和 1W 张狗的图片。但是如果我们现在做狗和狼图片识别任务，那情况就有些变化了。我们还是能方便的得到 1W 张狗的图片，但是狼因为在生活中不怎么常见，所以在同样的数据采集成本下我们可能只得到 100 张甚至更少的狼的图片。

还有比如 CTR 任务中我们需要预测用户是否会对广告进行点击，通常情况下曝光一个广告用户点击的比率非常低，这里我们假如给 101 个用户曝光广告可能只有一个人点击，那么得到的正负样本比例就为 1:100。如果是更高层级的广告转化目标比如下载、付费等正负样本的比例就更低了。

同样的例子会出现在文本分类任务中，假如我们要做一个识别是否对传奇游戏标签感兴趣的文本二分类器，用户搜索中这部分的比例非常少，也许 1W 条用户搜索 query 中只有 50 条甚至更少的样本属于正例。这种现象就是样本不均衡，因为样本会呈现一个长尾分布，头部的标签包含了大量的样本，而尾部的标签拥有很少的样本，就像下面这张图片中表现的那样出现一个长长的尾巴，所以这种现场也称为长尾现象。

![](https://pic4.zhimg.com/v2-ffbd39d5b3e848fcbc3d392df3a3a617_r.jpg)

1.2 样本不均衡可能带来的问题

上面讲了样本不均衡的现象在机器学习场景中经常出现，那么样本不均衡会带来什么问题呢？众所周知模型训练的本质是最小化损失函数，当某个类别的样本数量非常庞大，损失函数的值大部分被样本数量较大的类别所影响，导致的结果就是模型分类会倾向于样本量较大的类别。咱们拿上面文本分类的例子来说明，现在有 1W 条用户搜索的样本，其中 50 条和传奇游戏标签有关，9950 条和传奇游戏标签无关，那么模型全部将样本预测为负例，也能得到 99.5% 的准确率，会让人有一种模型效果还不错的假象，但是实际的情况是这个模型根本没什么卵用，因为我们的目标是识别出这些数量较少的类别。这也是我们在实际业务场景中遇到的问题。

总体来看，解决样本不均衡的问题主要从数据层面和模型层面来解决，下面会分别从理论到实践的角度分享样本不均衡问题的解决策略。 

**02 从模型层面解决样本不均衡问题**

本节主要从模型层面解决样本不均衡的问题。相比于控制正负样本的比例，我们还可以通过控制 Loss 损失函数来解决样本不均衡的问题。拿二分类任务来举例，通常使用交叉熵来计算损失，下面是交叉熵的公式：
$$
C E=\left\{\begin{array}{rll}
-\log (p), & \text { if } & y=1 \\
-\log (1-p), & \text { if } & y=0
\end{array}\right.
$$
上面的公式中 y 是样本的标签，p 是样本预测为正例的概率。

2.1 类别加权 Loss

为了解决样本不均衡的问题，最简单的是基于类别的加权 Loss，具体公式如下：
$$
C E=\left\{\begin{array}{rlr}
-\alpha \log (p), & \text { if } & y=1 \\
-(1-\alpha) \log (1-p), & \text { if } & y=0
\end{array}\right.
$$
基于类别加权的 Loss 其实就是添加了一个参数 a，这个 a 主要用来控制正负样本对 Loss 带来不同的缩放效果，一般和样本数量成反比。还拿上面的例子举例，有 100 条正样本和 1W 条负样本，那么我们设置 a 的值为 10000/10100，那么正样本对 Loss 的贡献值会乘以一个系数 10000/10100，而负样本对 Loss 的贡献值则会乘以一个比较小的系数 100/10100，这样相当于控制模型更加关注正样本对损失函数的影响。通过这种基于类别的加权的方式可以从不同类别的样本数量角度来控制 Loss 值，从而一定程度上解决了样本不均衡的问题。

2.2 Focal Loss

上面基于类别加权 Loss 虽然在一定程度上解决了样本不均衡的问题，但是实际的情况是不仅样本不均衡会影响 Loss，而且样本的难易区分程度也会影响 Loss。在样本不均衡的场景中，有非常多的负样本是易区分样本。虽然这些样本的 Loss 很低，但是数量确很多，所以对于最终的 Loss 有很大的贡献，导致模型最终的效果不够好。基于这个问题 2017 年何恺明大神在论文《Focal Loss for Dense Object Detection》中提出了非常火的 Focal Loss，下面是 Focal Loss 的计算公式：
$$
F L=\left\{\begin{array}{cll}
-\alpha(1-p)^{\gamma} \log (p), & \text { if } & y=1 \\
-(1-\alpha) p^{\gamma} \log (1-p), & \text { if } & y=0
\end{array}\right.
$$
相比于公式 2 来说，Focal Loss 添加了参数γ从置信的角度来加权 Loss 值。假如γ设置为 0，那么公式 3 蜕变成了基于类别的加权也就是公式 2；下面重点看看如何通过设置参数 r 来使得简单和困难样本对 Loss 的影响。当γ设置为 2 时，对于模型预测为正例的样本也就是 p>0.5 的样本来说，如果样本越容易区分那么 (1-p) 的部分就会越小，相当于乘了一个系数很小的值使得 Loss 被缩小，也就是说对于那些比较容易区分的样本 Loss 会被抑制，同理对于那些比较难区分的样本 Loss 会被放大，这就是 Focal Loss 的核心：**通过一个合适的函数来度量简单样本和困难样本对总的损失函数的贡献。** 关于参数γ的设置问题，Focal Loss 的作者建议设置为 2。下面是不同的参数值γ样本难易程度对 Loss 的影响对比图：

![](https://pic1.zhimg.com/v2-9a7899db89ec0d357538b172cbc5b118_b.jpg)

下面是一个 Focal Loss 的实现，感兴趣的小伙伴可以试试，看能不能对下游任务有积极效果：

![](https://pic3.zhimg.com/v2-c2009695d55883c5d55c457d3ceb83e6_r.jpg)

2.3 GHM Loss Focal Loss

主要结合样本的难易区分程度来解决样本不均衡的问题，使得整个 Loss 的曲线平滑稳定的下降，但是对于一些特别难区分的样本比如离群点会存在问题。可能一个模型已经收敛训练的很好了，但是因为一些比如标注错误的离群点使得模型去关注这些样本，反而降低了模型的效果。比如下面的离群点图：

![](https://pic2.zhimg.com/v2-c5dcc8da2004e29c8f81ea4b9426d219_b.jpg)

针对 Focal Loss 存在的问题，2019 年论文《Gradient Harmonized Single-stage Detector》中提出了 GHM(gradient harmonizing mechanism) Loss。相比于 Focal Loss 从置信度的角度去调整 Loss，GHM Loss 则是从一定范围置信度 p 的样本数量 (论文中称为梯度密度) 去调整 Loss。

理解 GHM Loss 的第一步是先理解梯度模长的概念，梯度模长 g 的计算公式如下：
$$
g=\left|p-p^{*}\right|=\left\{\begin{array}{ll}
1-p & \text { if } p^{*}=1 \\
p & \text { if } p^{*}=0
\end{array}\right.
$$
公式 4 中 p 代表模型预测为 1 的概率值，p * 是标签值。也就是说如果样本越难区分，那么 g 的值就越大。下面看看梯度模长 g 和样本数量的关系图：

![](https://pic3.zhimg.com/v2-6a960b1f63140fd57477c1b3303c8d4e_b.jpg)

从上图中可以看出样本中有很大一部分是容易区分的样本，也就是梯度模长 g 趋于 0 的部分。但是还存在一些十分困难区分的样本，也就是上图中右边红圈中的样本。**GHM Loss 认为不仅要降低对易区分样本的关注（这点和 Focal Loss 一致），而且还要降低十分困难样本的关注，因为这部分样本可能是标注错误的离群点，过多的关注这部分样本不仅不会提升模型的效果，反而还会有一定的逆向效果**。那么问题来了，怎么同时抑制容易区分的样本和十分困难区分的样本呢？ 针对这个问题，**从上图中可以发现容易区分的样本和十分困难区分的样本都存在一个共同点：数量多**。那么只要我们抑制一定梯度范围内数量多的样本就可以达到这个效果，GHM Loss 通过梯度密度 GD(g) 来表示一定梯度范围内的样本数量。这个其实有点像物理学中的密度，一定体积的物体的质量。梯度密度 GD(G) 的公式如下：
$$
G D(g)=\frac{1}{l_{\epsilon}(g)} \sum_{k=1}^{N} \delta_{\epsilon}\left(g_{k}, g\right)
$$
公式 5 中![](https://www.zhihu.com/equation?tex=%5Cdelta_+%7B%5Cvarepsilon%7D%28g_%7Bk%7D%2Cg%29) 代表样本中梯度模长 g 分布在![](https://www.zhihu.com/equation?tex=%5Cleft%28g-%5Cfrac%7B%5Cvarepsilon%7D%7B2%7D%2C+g%2B%5Cfrac%7B%5Cvarepsilon%7D%7B2%7D%5Cright%29) 范围里面的样本的个数， ![](https://www.zhihu.com/equation?tex=l_%7B%5Cepsilon%7D%28g%29) 代表了 ![](https://www.zhihu.com/equation?tex=%5Cleft%28g-%5Cfrac%7B%5Cvarepsilon%7D%7B2%7D%2C+g%2B%5Cfrac%7B%5Cvarepsilon%7D%7B2%7D%5Cright%29) 区间的长度。公式里面的细节小伙伴们可以去论文里面详细了解。

说完了梯度密度 GD(g) 的计算公式，下面就是 GHM Loss 的计算公式：
$$
\begin{aligned}
L_{G H M-C} &=\frac{1}{N} \sum_{i=1}^{N} \beta_{i} L_{C E}\left(p_{i}, p_{i}^{*}\right) \\
&=\sum_{i=1}^{N} \frac{L_{C E}\left(p_{i}, p_{i}^{*}\right)}{G D\left(g_{i}\right)}
\end{aligned}
$$
公式 6 中的 Lce 其实就是交叉熵损失函数，也就是公式 1。 下面看看交叉熵损失函数、Focal Loss 和 GHM Loss 三种损失函数对不同梯度模长样本的抑制效果图：

![](https://pic3.zhimg.com/v2-b0356d0259c99bef665861bcad9b9106_r.jpg)

从上图中可以看出交叉熵损失函数基本没有抑制效果，Focal Loss 可以有效的抑制容易区分的样本，而 GHM Loss 不仅可以很好的抑制简单样本，还能抑制十分困难的样本。 下面是复现了 GHM Loss 的一个 github 上工程，有兴趣的小伙伴可以试试： [https://github.com/libuyu/GHM_Detection](https://github.com/libuyu/GHM_Detection)

**03 从数据层面解决样本不均衡问题**  

现在我们遇到样本不均衡的问题，假如我们的正样本只有 100 条，而负样本可能有 1W 条。如果不采取任何策略，那么我们就是使用这 1.01W 条样本去训练模型。**从数据层面解决样本不均衡的问题核心是通过人为控制正负样本的比例，分成欠采样和过采样两种。**  
3.1 欠采样

欠采样的基本做法是这样的，现在我们的正负样本比例为 1:100。如果我们想让正负样本比例不超过 1:10，那么模型训练的时候数量比较少的正样本也就是 100 条全部使用，而负样本随机挑选 1000 条，这样通过人为的方式我们把样本的正负比例强行控制在了 1:10。这种方式存在一个问题，为了强行控制样本比例我们生生的舍去了那 9000 条负样本，这对于模型来说是莫大的损失。

相比于简单的对负样本随机采样的欠采样方法，实际工作中我们会使用迭代预分类的方式来采样负样本。具体流程如下图所示：

![](https://pic4.zhimg.com/v2-30ecd953c2d11a2ef39cad6f3ec4d48b_r.jpg)

首先我们会使用全部的正样本和从负例候选集中随机采样一部分负样本（这里假如是 100 条）去训练第一轮分类器；然后用第一轮分类器去预测负例候选集剩余的 9900 条数据，把 9900 条负例中预测为正例的样本（也就是预测错误的样本）再随机采样 100 条和第一轮训练的数据放到一起去训练第二轮分类器；同样的方法用第二轮分类器去预测负例候选集剩余的 9800 条数据，直到训练的第 N 轮分类器可以全部识别负例候选集，这就是使用迭代预分类的方式进行欠采样。

相比于随机欠采样来说迭代预分类的欠采样方式能最大限度的利用负样本中差异性较大的负样本，从而在控制正负样本比例的基础上采样出了最有代表意义的负样本。

欠采样的方式整体来说或多或少的会损失一些样本，**对于那些需要控制样本量级的场景下比较合适**。如果没有严格控制样本量级的要求那么下面的过采样可能会更加适合你。  
3.2 过采样

过采样和上面的欠采样比较类似，都是人工干预控制样本的比例，不同的是**过采样不会损失样本**。还拿上面的例子，现在有正样本 100 条，负样本 1W 条，最简单的过采样方式是我们会使用全部的负样本 1W 条，但是为了维持正负样本比例，我们会从正样本中有放回的重复采样，直到获取了 1000 条正样本，也就是说有些正样本可能会被重复采样到，这样就能保持 1:10 的正负样本比例了。这是最简单的过采样方式，这种方式可能会存在严重的过拟合。

实际的场景中会通过样本增强的技术来增加正样本。之前组里的小伙伴分享了基于 SMOTE 算法 (SMOTE 算法的本质是通过对训练集里的正例进行插值来产生额外的正例) 来增加正样本，感兴趣的小伙伴们可以关注下。在文本分类场景中我们主要通过样本增强技术来增加正样本。之前分享过一篇关于样本增强技术的文章

[数据拾光者：广告行业中那些趣事系列 13：NLP 中超实用的样本增强技术​zhuanlan.zhihu.com![图标](https://pic2.zhimg.com/v2-cc811fb613d1f687b9e65dbaae6b557d_180x120.jpg)](https://zhuanlan.zhihu.com/p/148836978)

里面包含了回译技术、替换技术、随机噪声引入技术等方法可以实现样本增强，通过这种方式可以增加正样本，并且使得增加的正样本不仅仅是简单的重复样本，而是有细微差异的正样本，这里不再赘述。因为之前还没接触过文本生成，所以介绍的方法里通过文本生成来增强样本的部分比较少。最近参加了公司的比赛，了解了一些文本生成的技术，增加点这段时间学到的通过文本生成的角度来实现文本增强的知识和大家分享下。

从文本生成的角度来增加正样本从而间接的使用过采样的方式来控制正负样本比例主要尝试过基于 BERT 的有条件生成任务和基于 SIMBERT 来生成相似文本任务：

(1) 基于 BERT 的有条件生成文本  
基于 BERT 的有条件生成任务主要是利用微软提供的 UNILM 来将 BERT 改造成可以处理 Seq2Seq 的任务，从而完成文本生成任务。下面是我用广告文案语料微调 BERT 从而生成的部分标签的文案的结果数据：

![](https://pic4.zhimg.com/v2-121ae9df9cf4a31667595b532106c9ff_b.jpg)

从上图中可以发现生成的文本其实和我们使用的文案语料很相似，但是又不完全相同。模型通过微调训练语料学习到了各个标签的知识，然后运用这些知识生成了相似的文案，这些文案虽然只有部分修改，但是语义是比较合理的，所以生成的结果也比较合理。这块使用的是苏剑林的 bert4keras 中的例子，有兴趣的小伙伴可以自己跑来玩一下：  
[https://github.com/bojone/bert4keras/blob/2accce143a9b7b6164a1a3a1a38507fc03788ec9/examples/task_conditional_language_model.py](https://github.com/bojone/bert4keras/blob/2accce143a9b7b6164a1a3a1a38507fc03788ec9/examples/task_conditional_language_model.py)  
(2) 基于 SIMBERT 生成相似文本  
基于 SIMBERT 生成相似文本的方法是另外一种文本生成的方式，主要原理是生成和当前 query 比较相似的文本从而达到样本增强的目的。下面是我们使用 SIMBERT 来生成相似文本的结果：

![](https://pic3.zhimg.com/v2-4f3745240d285163ca6a3635d7f37842_r.jpg)

上图中我们输入的初始文本是 “想开一家奶茶店，需要多少的预算？”，然后下面就是自动生成的相似的文本。可以发现生成的结果还不赖，整体语义一致，而文本的形式会有些许不同，从而达到了样本增强的目的。这块使用的也是苏剑林的 bert4keras 中的例子，感兴趣的小伙伴可以去试试： 
[https://github.com/bojone/bert4keras/blob/2accce143a9b7b6164a1a3a1a38507fc03788ec9/examples/basic_simple_web_serving_simbert.py](https://github.com/bojone/bert4keras/blob/2accce143a9b7b6164a1a3a1a38507fc03788ec9/examples/basic_simple_web_serving_simbert.py) 

**04 其他解决样本不均衡问题的策略**  

上面主要是从数据层面和模型损失函数来解决样本不均衡的问题，下面是一些其他的样本不均衡策略：

4.1 调节阈值修改正负样本比例

通常情况下 Sigmoid 函数会将大于 0.5 的阈值预测为正样本。这时候我们可以通过调节阈值来调整正负样本比例，比如设置 0.3 分作为阈值，将大于 0.3 的样本都判定为正样本，这样相当于增加了正样本的比例。  
4.2 利用半监督或自监督学习解决样本不均衡

之前领导分享过一篇知乎高赞的文章，主要是通过半监督或自监督学习来解决样本不均衡的问题，因为篇幅有限，这里就不详细介绍。后面可能会专门出一篇文章来详细讲解这种策略。这里先把链接放在这里，有兴趣的小伙伴也可以学习下：  
NeurIPS 2020 | 数据类别不平衡 / 长尾分布？不妨利用半监督或自监督学习  
[https://zhuanlan.zhihu.com/p/259710601?utm_source=wechat_session&utm_medium=social&utm_oi=27198249500672#ref_6](https://zhuanlan.zhihu.com/p/259710601?utm_source=wechat_session&utm_medium=social&utm_oi=27198249500672#ref_6) 

**总结**  

本篇主要从理论到实践解决文本分类中的样本不均衡问题。首先讲了下什么是样本不均衡现象以及可能带来的问题；然后重点从数据层面和模型层面讲解样本不均衡问题的解决策略。数据层面主要通过欠采样和过采样的方式来人为调节正负样本比例，模型层面主要是通过加权 Loss，包括基于类别 Loss、Focal Loss 和 GHM Loss 三种加权 Loss 函数；最后讲了下其他解决样本不均衡的策略，可以通过调节阈值修改正负样本比例和利用半监督或自监督学习解决样本不均衡问题。需要说明下上面解决样本不均衡问题的策略不仅仅适用于文本分类任务，还可以扩展到其他的机器学习任务中。对于希望解决样本不均衡问题的小伙伴可能有所帮助。 

**参考资料** 
[1] 《Focal Loss forDense Object Detection》 
[2]《GradientHarmonized Single-stage Detector》
---
title: Newton method require fewer iterations than gradient descent
categories:
- Optimization
tags:
- Newton method
- gradient descent
date: 2021/3/10 20:00:17
updated: 2021/3/12 12:00:17
---

牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。

根据 wiki 上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。

<img src="https://gitee.com/gaoyi-ai/image-bed/raw/master/images/365e99bcf8d2e1ef1986e09c795caef7_hd.jpg" style="zoom:50%;" />红色的牛顿法的迭代路径，绿色的是梯度下降法的迭代路径。

> [Newton's_method_in_optimization](http://en.wikipedia.org/wiki/Newton's_method_in_optimization)

---

梯度下降法是一次收敛没问题，**实际中用的牛顿法严格意义上不是二次收敛，因为步长设定的关系**  
ref:[Convex Optimization](http://stanford.edu/~boyd/cvxbook/)

为了方便分析两个问题的收敛的性能，给出如下两个条件  
1. 假设问题是**无约束凸优化问题**$minimize+f(x)$，即求凸函数$f(x)$的最小值。凸优化问题是应用最广的优化模型，因为它最容易解决，实际应用中的很多问题都是能转化为凸优化问题的。令最优解为$x^{*}+$，最优值为$p^{*}+=+f(x+^+{*})$  
2. 进一步，假设目标函数在可行域上是**强凸**的，即存在$m,M>0$，使得$mI+\preceq+\nabla^{2}+f(x)+\preceq+MI$成立

设某次迭代开始时的近似解为$x$，那么只要没有达到最优解，本次迭代就应该产生一个新的近似解$x^{+}=x+t\Delta x$，其中$\Delta x$表示搜索方向，$t$是一个标量，表示步长。一般来说，$t$是在算法开始时就给定的，也就是每次迭代都不变，这种方法叫**精确直线搜索**，但更有效的方式应该是**回溯直线搜索**，也就是在每次迭代时都重新算出一个$t$，稍后我会简单说明这个方法，先按下不表。我们讨论的都是下降方法，那么应该有$f(x^{+})<f(x)$，进一步根据函数的凸性可以得到$\nabla f(x)^{T} \Delta x<0$，这是任意一种优化方法必须满足的条件

梯度下降法和牛顿法的区别是在$x$附近选择了不同阶模型的近似。设$x$附近的近似模型为$\tilde{f}(x+v)$，其中如果选择一阶近似，则有$f(x+v)\approx \tilde{f} (x+v)=f(x) +\nabla f(x)^T v$，只要$\nabla f(x)^{T}v$是负的那就是下降方法，可以看出$\nabla f(x)^{T}v$是$v$的线性函数，只要$v$选对了方向$v$的长度越大那$\nabla f(x)^{T}v$就越小，最后就是$-\infty$，这个问题就没有意义了。所以我们必须得规范$v$的长度，范数$\|\cdot\|$可以起到这个作用。所以我们可以定义一个规范化的下降方向，如下：  
$\Delta x_{nsd}=argmin\{\nabla f(x)^{T}v:\|v\|=1\}$

它的几何意义是单位球上在$-\nabla f(x)$方向上延伸最远的向量，这种方法叫做**最速下降法**，如图  

![](https://gitee.com/gaoyi-ai/image-bed/raw/master/images/8c81b179fcccc344f7eb8c6a947af800_hd.jpg)  

>[www.zhihu.com/question/19723347](https://www.zhihu.com/question/19723347)

---

1. 牛顿法起始点不能离局部极小点太远，否则很可能不会收敛。(考虑到二阶拟合应该很容易想象)，所以实际操作中会先使用别的方法，比如梯度下降法，使更新的点离最优点比较近，再开始用牛顿法。  
2. 牛顿法每次需要更新一个二阶矩阵，当维数增加的时候是非常耗内存的，所以实际使用是会用拟牛顿法。  
3. 梯度下降法在非常靠近最优点时会有震荡，就是说明明离的很近了，却很难到达，因为线性的逼近非常容易一个方向过去就过了最优点 (因为只能是负梯度方向)。但牛顿法因为是二次收敛就很容易到达了。

牛顿法最明显快的特点是对于二阶函数 (考虑多元函数的话要在凸函数的情况下)，牛顿法能够一步到达，非常有效。

---

如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）

从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。

## 梯度下降法和高斯牛顿法

首先梯度下降法和高斯牛顿法都是最优化方法。

其区别之处在于，梯度下降法在寻找目标函数极小值时，是沿着反梯度方向进行寻找的。梯度的定义就是指向标量场增长最快的方向，在寻找极小值时，先随便定初始点（x0，y0）然后进行迭代不断寻找直到梯度的模达到预设的要求。但是梯度下降法的缺点之处在于：在远离极小值的地方下降很快，而在靠近极小值的地方下降很慢。

而高斯牛顿法是一种非线性最小二乘最优化方法。其利用了目标函数的泰勒展开式把非线性函数的最小二乘化问题化为每次迭代的线性函数的最小二乘化问题。高斯牛顿法的缺点在于：若初始点距离极小值点过远，迭代步长过大会导致迭代下一代的函数值不一定小于上一代的函数值。所以在高斯牛顿法中加入了因子μ，当μ大时相当于梯度下降法，μ小时相当于高斯牛顿法。（这儿我也不明白为什么，希望有大牛解答）。在使用Levenberg-Marquart时，先设置一个比较小的μ值，当发现目标函数反而增大时，将μ增大使用梯度下降法快速寻找，然后再将μ减小使用牛顿法进行寻找。


---
title: Quasi-Newton
categories:
- Optimization
tags:
- Quasi-Newton
- Newton
date: 2021/3/17 10:00:00
updated: 2021/3/17 16:00:00
---

# 牛顿法和拟牛顿法

牛顿法（Newton method）和拟牛顿法（quasi Newton method）是求解无约束最优化问题的常用方法，收敛速度快。牛顿法是迭代算法，每一步需要求解Hessian矩阵的逆矩阵，计算比较复杂。拟牛顿法通过正定矩阵近似Hessian矩阵的逆矩阵或Hessian矩阵，简化了这一计算过程。

## 牛顿法

我们假设点 x∗为函数 f(x) 的根，那么有 f(x∗)=0。现在我们把函数 f(x) 在点 xk 处一阶泰勒展开有：
$$
f(x)  = f(x_k) + f^\prime(x_k)(x-x_k)
$$
假设点 $x_{k+1}$ 为该方程的根，则有：
$$
f(x_{k+1}) = f(x_k) + f^\prime(x_k)(x_{k+1}-x_k) = 0
$$
可以得到
$$
x_{k+1} = x_k - \frac{f(x_k)}{f^\prime(x_k)}
$$
这样我们就得到了一个递归方程，我们可以通过迭代的方式不断的让 x 趋近于 x∗从而求得方程 f(x) 的解。

### 最优化问题

对于最优化问题，其极值点处一阶导数为 0。因此我们可以在一阶导数处利用牛顿法通过迭代的方式来求得最优解，即相当于求一阶导数对应函数的根。 

首先，我们对函数在 x0 点处进行二阶泰勒展开
$$
f(x)=f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\frac{1}{2} f^{\prime \prime}\left(x_{0}\right)\left(x-x_{0}\right)^{2}
$$
对 x 求导可得
$$
f^{\prime}(x)=f^{\prime}\left(x_{0}\right)+f^{\prime \prime}\left(x_{0}\right)\left(x-x_{0}\right)
$$
由于在极值点处 $f^{\prime}(x)=0$ ，于是

$$
f^{\prime}\left(x_{0}\right)+f^{\prime \prime}\left(x_{0}\right)\left(x-x_{0}\right)=0
$$
从而可以得出下一个 x 的位置

$$
x=x_{0}-\frac{f^{\prime}\left(x_{0}\right)}{f^{\prime \prime}\left(x_{0}\right)}
$$
其迭代形式为

$$
x_{k+1}=x_{k}-\frac{f^{\prime}\left(x_{k}\right)}{f^{\prime \prime}\left(x_{k}\right)}, \quad k=0,1, \cdots
$$
对于多维函数，二阶导数就变成了一个海森矩阵，二阶泰勒展开公式如下:

$$
f(x)=f\left(x_{0}\right)+\nabla f\left(x_{0}\right) \cdot\left(x-x_{0}\right)+\frac{1}{2} \cdot\left(x-x_{0}\right)^{T} \cdot \nabla^{2} f\left(x_{0}\right) \cdot\left(x-x_{0}\right)
$$

$$
\nabla^{2} f=\left[\begin{array}{cccc}\frac{\partial^{2} f}{\partial x_{1}^{2}} & \frac{\partial^{2} f}{\partial x_{1} \partial x_{2}} & \cdots & \frac{\partial^{2} f}{\partial x_{1} \partial x_{N}} \\ \frac{\partial^{2} f}{\partial x_{2} \partial x_{1}} & \frac{\partial^{2} f}{\partial x_{2}^{2}} & \ldots & \frac{\partial^{2} f}{\partial x_{2} \partial x_{N}} \\ \frac{\partial^{2} f}{\partial x_{N} \partial x_{1}} & \frac{\partial^{2} f}{\partial x_{N} \partial x_{2}} & \cdots & \frac{\partial^{2} f}{\partial x_{N}^{2}}\end{array}\right]
$$

图中的 $\nabla^{2} f$ 便是海森矩阵 $H(x)=\left[\frac{\partial^{2} f}{\partial x_{i} \partial x_{j}}\right]_{n \times n}$ 。

迭代公式就变成了 $x^{k+1}=x^{k}-H_{k}^{-1} f_{k}^{\prime}$ 。

我们可以看到，当  $\mathrm{Hk}$ 为正定（ $\mathrm{Hk}-1$ 也正定 ）的时候，可以保证牛顿法的搜索方向是向下搜索的。 

## 拟牛顿法

当特征特别多的时候，求海森矩阵的逆矩阵，运算量是非常大且慢，考虑用一个 n 阶矩阵来替代，这就是拟牛顿法的基本思路。

通过以上可以得到
$$
\begin{array}{l}
f^{\prime}\left(x^{k+1}\right)=f^{\prime}\left(x^{k}\right)+H_{k}\left(x^{k+1}-x^{k}\right) \Rightarrow \\
H_{K}^{-1}\left(f^{\prime}\left(x^{k+1}\right)-f^{\prime}\left(x_{k}\right)\right)=x^{k+1}-x^{k}
\end{array}
$$
因此，对于我们所选择的替代矩阵 Gk，需要满足两个条件：

1.  拟牛顿条件，即 $G_{k}\left(f^{\prime}\left(x^{k+1}\right)-f^{\prime}\left(x_{k}\right)\right)=x^{k+1}-x^{k}$ 
2.  要保证 Gk 为正定矩阵，这是因为只有正定才能保证搜索方向是向下搜索的。

由于每次迭代都需要更新矩阵 Gk ，$G_{k+1}=G_{k}+\Delta G_{k}$，下面介绍一些方法。

###  DFP

$D_{k+1}=D_{k}+\Delta D_{k}, \quad k=0,1,2, \cdots \quad,$，这里的 $D_k$ 相当于前边提到的 $G_k$，这个迭代公式的关键是每一步的校正矩阵$ΔD_k$ 的构造。我们采用待定法，先将$ΔD_k$ 待定为下面的形式:

$$
\Delta D_{k}=\alpha u u^{T}+\beta v v^{T}
$$
α和β为待定系数，u 和 v 为待定向量。$uu^T$ 和 $vv^T$ 均为对称矩阵，因此可以保证$ΔD_k$ 也是对称矩阵。 
将待定公式代入迭代公式，可得: $\mathrm{D_{k+1}} \cdot \mathrm{yk}=\mathrm{sk}$
$$
\begin{aligned}
s_{k} &=D_{k} y_{k}+u\left(\alpha u^{T} y_{k}\right)+v\left(\beta v^{T} y_{k}\right) \\
&=D_{k} y_{k}+\left(\alpha u^{T} y_{k}\right) u+\left(\beta v^{T} y_{k}\right) v
\end{aligned}
$$
上述变化是因为 $\alpha u^{T} y_{k}, \beta v^{T} y_{k} \quad$ 是两个数，不妨设为$$\alpha u^{T} y_{k}=1, \beta v^{T} y_{k}=-1$$ 

可得 $\alpha=\frac{1}{u^{T} y_{k}}, \beta=-\frac{1}{v^{T} y_{k}}$ 

将其带入 sk 的表达式，可得 $\quad u-v=s_{k}-D_{k} y_{k}$ 

不妨直接取$u=s_{k}, v=D_{k} y_{k}$ 

代回上面α和β的表达式，得:

$$
\alpha=\frac{1}{s_{k}^{T} y_{k}}, \beta=-\frac{1}{\left(D_{k} y_{k}\right)^{T} y_{k}}=-\frac{1}{y_{k}^{T} D_{k} y_{k}}
$$
将上两式代回$ΔD_k$ 的表达式，得

$$
\Delta D_{k}=\frac{s_{k} s_{k}^{T}}{s_{k}^{T} y_{k}}-\frac{D_{k} y_{k} y_{k}^{T} D_{k}}{y_{k}^{T} D_{k} y_{k}}
$$
这样就可以使用迭代公式来进行拟牛顿法的计算了。

### BFGS

可以考虑用 Gk 逼近海森矩阵的逆矩阵 H-1，也可以考虑用 Bk 逼近海森矩阵 H。

BFGS 算法的迭代公式: $B_{k+1}=B_{k}+\Delta B_{k}, \quad k=0,1,2, \cdots$

由于拟牛顿条件可以写成$y_{k}=B_{k+1} \cdot s_{k}$或$s_{k}=D_{k+1} \cdot y_{k}$ ，同上述 DFP 的推导过程一样，只不过是$y_{k}=B_{k} s_{k}+\left(\alpha u^{T} s_{k}\right) u+\left(\beta v^{T} s_{k}\right) v$

最终可得

$$
B_{k+1}=B_{k}+\frac{y_{k} y_{k}^{T}}{y_{k}^{T} s_{k}}-\frac{B_{k} s_{k} s_{k}^{T} B_{k}}{s_{k}^{T} B_{k} s_{k}}
$$

### Broyden 类算法

我们可以从 BFGS 算法矩阵 Bk 的迭代公式得到 BFGS 算法关于 Gk 的迭代公式。事实上，若记 $G_{k}=B_{k}^{-1}, \quad G_{k+1}=B_{k+1}^{-1},$那么两次应用 ShermanMorrisn 公式可得

$$
G_{k+1}=\left(I-\frac{\delta_{k} y_{k}^{\mathrm{T}}}{\delta_{k}^{\mathrm{T}} y_{k}}\right) G_{k}\left(I-\frac{\delta_{k} y_{k}^{\mathrm{T}}}{\delta_{k}^{\mathrm{T}} y_{k}}\right)^{\mathrm{T}}+\frac{\delta_{k} \delta_{k}^{\mathrm{T}}}{\delta_{k}^{\mathrm{T}} y_{k}}
$$
称为 BFGS 算法关于 Gk 的迭代公式。

由 DFP 和 BFGS 算法得到的迭代公式 GK，满足拟牛顿条件式，所以它们的线性组合 $G_{k+1}=\alpha G^{\mathrm{DFP}}+(1-\alpha) G^{\mathrm{BFGS}}$ 也满足拟牛顿条件式，而且是正定的。其中 0 ≤ a ≤ 1。

这样就得到了一类拟牛顿法，称为 Broyden 类算法。 

注：Sherman-Morrison 公式：假设 A 是 n 阶可逆矩阵，u,v 是 n 维向量，且 A+uvT 也是可逆矩阵，则
$$
\left(A+u v^{\top}\right)^{-1}=A^{-1}-\frac{A^{-1} u v^{T} A^{-1}}{1+v^{T} A^{-1} u}
$$

## 算法的数值特性

利用 $𝐵_𝑘$ 求 $𝑑_𝑘$ 的计算量为 $𝑂(𝑛^3)$ ，而利用 $𝐻_𝑘$ 求 $𝑑_𝑘$ 的计算量仅为 $𝑂(𝑛^2)$ ，为何要保留利用 $𝐵_𝑘$ 计算的方法呢？

1. 由于计算中舍入误差的存在，会出现 $𝐻_{𝑘+1}$ 不正定甚至奇异的情况。
2. 而且我们不一定能够发现这个问题。
• 我们利用 $B_{k+1}$ 来进行计算是因为：
• 可以及时发现并改正 $B_{k+1}$ 不正定的问题。

## 解方程组

• 在解关于 $B_k$ 的线性方程组时， $B_k$ 要进行 $𝐿𝐷𝐿^𝑇$ 分解，即： $B_k$ = $𝐿_𝑘𝐷_𝑘𝐿_𝑘^𝑇$
• 其中 $L_k$ 为单位下三角阵， $𝐷_𝑘$ 为对角阵，可以利用DFP公式或BFGS公式修正 $𝐷_𝑘$ 和 $L_k$ 得到 $𝐷_{𝑘+1}$ 和 $𝐿_{𝑘+1}$ ，即： $𝐿_{𝑘+1} = 𝐿_𝑘 + ∆𝐿_𝑘$, $𝐷_{𝑘+1} = 𝐷_𝑘 + ∆𝐷_𝑘$
 • 从而得到： $B_{k+1} = 𝐿_{𝑘+1}𝐷_{𝑘+1}𝐿_{𝑘+1}^𝑇$
• 这种做法可以免去每次迭代都对 $B_k$ 进行分解的工作，从而使利用 $B_k$ 求 𝑑𝑘 的计算量降低到 $𝑂(𝑛^2)$

## 如何保证 $B_k$ 的正定性

• 如果矩阵 $𝐷_{𝑘+1}$ 有对角元小于给定的 𝜀 > 0 ，则可用 𝜀 代替该对角元，从而保证 $B_{k+1}$ 在数值上是正定的。
• 要使对于任何非零向量 𝑑 均有$𝑑^𝑇𝐵𝑑 > 0$ ，我们把 𝐵 做 $𝐿𝐷 𝐿^𝑇$ 分解，有： $𝑑^𝑇𝐵𝑑 = 𝑑^𝑇𝐿𝐷 𝐿^𝑇𝑑 = (𝐿^𝑇𝑑)^𝑇𝐷(𝐿^𝑇𝑑)$

 • 令 $𝑥 = 𝐿^𝑇𝑑$ ，则上式可写为 $𝑥^𝑇𝐷𝑥$ ，令 $𝑥 = (𝑥_1, … , 𝑥_𝑛)^𝑇$, $𝐷 = 𝑑𝑖𝑎𝑔 (𝑑_1, … , 𝑑_𝑛)$
 • 因此 $𝑥^𝑇𝐷𝑥 = \sum_{𝑖=1}^𝑛 𝑑_𝑖x_𝑖^2$ ，因此只要保证所有的 𝑑𝑖 均大于0，就可以保证对于
任何向量 𝑑 均有 $𝑑^𝑇𝐵𝑑 > 0$ ，从而保证 $B_{k+1}$ 在数值上是正定的。
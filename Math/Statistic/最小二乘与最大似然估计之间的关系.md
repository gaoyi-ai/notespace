---
title: 最小二乘与最大似然估计之间的关系
categories:
- Math
- Statistic
tags:
- Least squares
- MLE
date: 2021/3/17 10:00:00
updated: 2021/3/17 16:00:00
---



# 最小二乘与最大似然估计之间的关系

## 1、结论

测量误差（测量）服从高斯分布的情况下， 最小二乘法等价于极大似然估计。

## 2、最大似然估计概念

$\max{L(\theta;X)}$   

最大似然估计就是通过求解最大的上式得到可能性最大的$\theta =$作为真$\theta$的估计，其中 L 函数称为参数$\theta$的似然函数，是一个概率分布函数。

  似然估计的思想是：测量值 X 是服从概率分布的，求概率模型中的参数，使得在假设的分布下获得该组测量出现概率最大：

例如：通过一次测量得到 1.9、1.9、2.0、2.1、2.0、1.9、1.5、2.5、2.0、2.0，

 通过直觉我们发现这组测量比较符合期望为 2 的高斯分布。要不然，为什么取值都在 2 附近呢，为啥测量数据中没有（很少）1.0、5.0 呢？也就是说，可以认为我的这些测量是符合某个概率分布的（这个例子中为高斯分布），测量值中得到 2 附近值的概率比较大。

例如：一个箱子里有红球和黑球，通过 10 次放回抽取实验得到的结果为：8 次黑球、2 次红球。问箱子中黑球的比例？这个例子中箱子里只有红球和黑球，可以假设黑球的比例为 p ，那么红球的概率为（1-p），那么 10 次实验中 8 次黑球、两次红球的概率为：$L(p;8,2)={p^8}*{(1-p)^2}$。通过$\max L$即可求出 p = 0.8。

## 3、最大似然估计求法

a、样本从**高斯分布中**采样获得。高斯概率分布函数为：

$$
f=\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
$$
通过测量获得 N 个测量值，$x_1...x_n$，它们符合高斯概率分布，此时它们的似然估计为：

$$
L\left(\mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{\left(x_{1}-\mu\right)^{2}}{2 \sigma^{2}}} * \ldots \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{\left(e_{n}-\mu\right)^{2}}{2 \sigma^{2}}}=\prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{\left(x_{i}-\mu\right)^{2}}{2 \sigma^{2}}}
$$
通过最大化似然估计函数 L ，即可求出$\mu,\sigma$：

$$
\max L\left(\mu, \sigma^{2}\right)
$$
但是，（4）式中目标函数为乘积的形式，求导结果复杂，这里对目标函数取对数，这样做不影响单调性：

$$
\log L\left(\mu, \sigma^{2}\right)=-\frac{n}{2} \log (2 \pi)-\frac{n}{2} \log \left(\sigma^{2}\right)-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}
$$
 (4) 中似然函数取对数后的函数称为似然函数，通过对似然函数取对数可以简化似然函数的求解。

对似然函数求导并等于 0，即可求出最大似然下的$\mu,\sigma$：

$$
\left\{\begin{array}{l}
\frac{\partial \log L\left(\mu, \sigma^{2}\right)}{\partial \mu}=\frac{1}{\sigma^{2}} \sum_{i=1}^{n}\left(x_{i}-\mu\right)=0 \\
\frac{\partial \log L\left(\mu, \sigma^{2}\right)}{\partial \sigma^{2}}=-\frac{n}{2 \sigma^{2}}+\frac{1}{2 \sigma^{1}} \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}=0
\end{array}\right.
$$
得到：

$$
\left\{\begin{array}{l}
\mu^{*}=\bar{x}=\frac{1}{n} \sum_{i=1}^{n} x_{i} \\
\sigma^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}
\end{array}\right.
$$
 c、最大对数似然估计一般求解过程：

写出最大似然估计表达式：

$$
\log L(\theta)=\sum_{i=1}^{n} \log f\left(x_{i} ; \theta_{1} \ldots \theta_{k}\right)
$$
对每个$\theta$求偏导并等于 0：

$$
\frac{\log L(\theta)}{\partial \theta_{i}}=0, i=1 \ldots k
$$
d、与最大似然估计对等的还有一个 **矩估计方法**。

矩估计法，是利用样本矩来估计总体中相应的参数。矩估计法的基本思想是用样本矩代替总体矩。

最简单的矩估计法是用一阶样本原点矩来估计总体的期望而用二阶样本中心矩来估计总体的方差。

## 4、最大似然估计与最小二乘之间的关系

从概率论的角度：

a、最小二乘（Least Square）的解析解可以用 Gaussian 分布以及最大似然估计求得

b、Ridge 回归可以用 Gaussian 分布和最大后验估计解释

c、LASSO 回归可以用 Lapace 分布和最大后验估计解释

假设线性回归**模型**具有如下形式：

$$
f(x)=\sum \limits_{j=1}^d x_j w_j+\varepsilon=x w^T+\varepsilon
$$
其中$x\in{R^{1{\times}{d}}}$，$w\in{R^{1{\times}{d}}}$误差$\varepsilon{\in}{R}$。

当前已知$X={({x_1}...{x_n})^T}{\in}{R^{n{\times}{d}}}$，$y{\in}{R^{n{\times}{1}}}$，如何求w呢？

**策略** 1：假设${\varepsilon_i}\sim{\rm{N}}(0,{\sigma^2})$，即${y_i}{\sim}N({x_i}{w^T},{\sigma^2})$那么用最大似然估计推导：
$$
\begin{aligned}
\arg \max _{w} L(w) &=\ln \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2}\left(\frac{y_{i}-x_{i} w^{T}}{\sigma}\right)^{2}\right.\\
\arg \max _{w} L(w) &=-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-x_{i} w^{T}\right)^{2}-n \ln \sigma \sqrt{2 \pi} \\
\arg \min _{w} f(w) &=\sum_{i=1}^{n}\left(y_{i}-x_{i} w^{T}\right)^{2}=\left\|y-X w^{T}\right\|_{2}^{2}
\end{aligned}
$$

上式就是最小二乘。

**策略** 2：假设${\varepsilon_i}\sim{\rm{N}}(0,{\sigma^2})$，${w_i}{\sim}N(0,{\tau^2})$那么用最大后验估计推导：
$$
\begin{array}{c}
\arg \max _{w} L(w)=\ln \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2}\left(\frac{y_{i}-x_{i} w^{T}}{\sigma}\right)^{2}\right. \\
\arg \max _{w} L(w)=-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-x_{i} w^{T}\right)^{2}-\frac{1}{2 \tau^{2}} \sum_{j=1}^{d} w_{j}^{2}-n \ln \sigma \sqrt{2 \pi}-d \ln \tau \sqrt{2 \pi} \\
\arg \min _{w} f(w)=\sum_{i=1}^{n}\left(y_{i}-x_{i} w^{T}\right)^{2}+\lambda \sum_{j=1}^{d} w_{j}^{2}=\left\|y-X w^{T}\right\|_{2}^{2}+\lambda\|w\|_{2}^{2}
\end{array}
$$

上式就是 Ridge 回归。

 策略 3：假设${\varepsilon_i}\sim{\rm{N}}(0,{\sigma^2})$，${w_i}{\sim}Lapace(0,b)$，同样采用后验估计推导：

$$
\begin{array}{c}
\arg \max _{w} L(w)=\ln \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2}\left(\frac{y_{i}-x_{i} w^{T}}{\sigma}\right)^{2}\right. \\
\arg \max _{w} L(w)=-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-x_{i} w^{T}\right)^{2}-\frac{1}{2 \tau^{2}} \sum_{j=1}^{d}\left|w_{j}\right|-n \ln \sigma \sqrt{2 \pi}-d \ln \tau \sqrt{2 \pi} \\
\arg \min _{w} f(w)=\sum_{i=1}^{n}\left(y_{i}-x_{i} w^{T}\right)^{2}+\lambda \sum_{j=1}^{d}\left|w_{j}\right|=\left\|y-X w^{T}\right\|_{2}^{2}+\lambda\|w\|_{1}
\end{array}
$$

上式 LASSO。